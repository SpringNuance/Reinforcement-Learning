{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8323a86e",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 6 - Actor Critic part 1 </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is part of the teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Sep 4, 2023 - Nov 30, 2023</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Learning Objectives </a>\n",
    "* <a href='#1.2'> 1.2 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Policy Gradient with a Critic </a>\n",
    "* <a href='#3.'> 3. Submitting </a>\n",
    "* <a href='#3.1'> 3.1 Feedback </a>\n",
    "* <a href='#4.'> References</a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implementing PG with critic (20 points)</a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> Relationship between actor-critic and REINFORCE with baseline (10 points)</a>\\\n",
    "<a href='#Q2'><b>Student Question 1.2</b> Advantage (5 points)  </a>\\\n",
    "<a href='#Q3'><b>Student Question 1.3</b> Bias and Variance Analysis (10 points) </a>\\\n",
    "<a href='#Q4'><b>Student Question 1.4</b> Controlling bias-variance tradeoff (10 points)</a>\n",
    "    \n",
    "**Total Points:** 55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53631b",
   "metadata": {},
   "source": [
    "# 1. Introduction <a id='1.'></a>\n",
    "\n",
    "In this assignment, we will implement an actor-critic reinforcement learning algorithm which combines elements of both value-based methods (critic) and policy-based methods (actor) to improve learning and stability in the **InvertedPendulum-v4**  environment.\n",
    "\n",
    "## 1.1 Task environments: <a id='1.1'></a>\n",
    "\n",
    "In this exercise, we will focus on InvertedPendulum-v4 tasks:\n",
    "- InvertedPendulum-v4(https://www.gymlibrary.dev/environments/mujoco/inverted_pendulum/): This environment is similar to the cartpole environment but now powered by the Mujoco physics simulator - allowing for more complex experiments (such as varying the effects of gravity). This environment involves a cart that moves horizontally, with a pole fixed on the cart at one end and the other end of the pole moving freely. The cart can be pushed left or right. The goal is to move the pole such that it is vertically above the cart pointing straight up by applying horizontal forces on the cart.\n",
    "<figure style=\"text-align: center\">\n",
    "    <img src=\"imgs/InvertedPendulum.png\" width=\"300\"/>\n",
    "    <figcaption style=\"text-align: center\">  Figure 1: The InvertedPendulum-v4 environment. </figcaption>\n",
    "</figure>\n",
    "\n",
    "## 1.2 Learning Objectives: <a id='1.1'></a>\n",
    "\n",
    "- Understand the idea of actor-critic algorithms\n",
    "- Understand the limits and use cases of actor-critics\n",
    "\n",
    "## 1.3 Code Structure & Files <a id='1.2'></a>\n",
    "\n",
    "```ex6_PG_AC.ipynb``` is the file needed to be modified for this part of the assignment.  \n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER PART IN ```ex6_DDPG.ipynb```** </span>\n",
    "\n",
    "```\n",
    "├───cfg                            # Config files for environments\n",
    "├───imgs                           # Images used in notebook\n",
    "├───results\n",
    "│   └───HalfCheetah-v4\n",
    "│   │   ├───logging                \n",
    "│   │   │    └───logging.pkl        # Contains logged data\n",
    "│   │   ├───model              \n",
    "│   │   │    └───*HalfCheetah-v4_params.pt    # Contains trained model\n",
    "│   │   └───video                   # Videos saved\n",
    "│   │   └───*ddpg.png               # Contains training performance plot\n",
    "│   └───InvertedPendulum-v4\n",
    "│   │   ├───logging                \n",
    "│   │   │    └───logging.pkl        # Contains logged data\n",
    "│   │   ├───model              \n",
    "│   │   │    └───*InvertedPendulum-v4_params.pt      # Contains trained model\n",
    "│   │   └───video                   # Videos saved\n",
    "│   │   └───*pg_ac.png              # Contains training performance plot\n",
    "│   ex6_DDPG.ipynb                  # 2nd assignment file containing tasks <---------\n",
    "│   ex6_PG_AC.ipynb                 # 1st assignment file containing tasks <---------This task\n",
    "│   train.py                        # Contains train and test functions \n",
    "│   utils.py                        # Contains useful functions \n",
    "└───buffer.py                       # Contains buffer functions\n",
    "```\n",
    "\n",
    "## 1.4 Execution time <a id='1.4'></a>\n",
    "\n",
    "The training of DDPG may take more than 15 mins depending on the server load. If you have problems with the training time, you can train locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb79df48",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Policy Gradient with a Critic <a id='2.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584d11d",
   "metadata": {},
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implement policy gradient (PG) with critic (20 points) </h3> \n",
    "\n",
    "Revisit the policy gradient solution for the InvertedPendulum from Exercise 5 with learned sigma if needed. Implement the actor-critic algorithm below. Perform TD(0) updates at the end of each episode. You can check the training performance plot in the result folder after running the plot cell. Take Figure 2 as a reference training plot. \n",
    "    \n",
    "**Hint:** Check out the PyTorch tutorial from Mycourses to see how to calculate the $A_\\theta \\Delta_\\theta \\log \\pi_\\theta(a_i|s_i)$ term using the ```detach()``` function. \n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"imgs/pg_ac.png\" width=\"400px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 2: Training plot of the policy gradient with a critic.\n",
    "</figcaption>\n",
    "</figure>\n",
    "     \n",
    "**Complete the all the unfinished implementation in `PG` class (marked with ```TODOs```)**. \n",
    "    \n",
    "1. **Policy Network**: Finish the `__init__(self, state_dim, action_dim)` function and `forward(self, state)` function within the `Policy` class\n",
    "2. **Agent Update Function**: Finish the `update(self, )` function within the `PG` class\n",
    "3. **Get Action Method**: Finish the `get_action(self, observation, evaluation=False)` function within the `PG` class.\n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad4384f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "import torch, yaml\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import Video\n",
    "\n",
    "\n",
    "import train as t\n",
    "import utils as u\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6634d41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Actor-critic agent\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        # Create a neural network and use it for the mean of the policy.\n",
    "        # The size of the neural network here has been chosen such that \n",
    "        # it is not too big but should perform well in the tasks we want to look at.\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_dim, 64)), nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)), nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, action_dim), std=0.01),\n",
    "        )\n",
    "        # TODO: Implement actor_logstd as a learnable parameter\n",
    "        # Use log of std to make sure std (standard deviation) of the policy\n",
    "        # doesn't become negative during training\n",
    "        #self.actor_logstd =\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Get mean of a Normal distribution (the output of the neural network)\n",
    "        action_mean = self.actor_mean(state)\n",
    "\n",
    "        # Make sure action_logstd matches dimensions of action_mean\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "\n",
    "        # Exponentiate the log std to get actual std\n",
    "        action_std = torch.exp(action_logstd)\n",
    "\n",
    "        # TODO: Create a Normal distribution with mean of 'action_mean' and standard deviation of 'action_logstd', and return the distribution\n",
    "        #probs = \n",
    "\n",
    "        return probs\n",
    "\n",
    "class Value(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.value = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_dim, 64)), nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)), nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.value(x).squeeze(1) # output shape [batch,]\n",
    "\n",
    "\n",
    "class PG(object):\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma):\n",
    "        self.name = 'pg'\n",
    "        self.policy = Policy(state_dim, action_dim).to(device)\n",
    "        self.value = Value(state_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(list(self.policy.parameters())+ list(self.value.parameters()), \n",
    "                                         lr=float(lr),)\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # a simple buffer\n",
    "        self.states = []\n",
    "        self.action_probs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.next_states = []\n",
    "\n",
    "\n",
    "    def update(self,):\n",
    "        action_probs = torch.stack(self.action_probs, dim=0) \\\n",
    "                .to(device).squeeze(-1)\n",
    "        rewards = torch.stack(self.rewards, dim=0).to(device).squeeze(-1)\n",
    "        states = torch.stack(self.states, dim=0).to(device).squeeze(-1)\n",
    "        next_states = torch.stack(self.next_states, dim=0).to(device).squeeze(-1)\n",
    "        dones = torch.stack(self.dones, dim=0).to(device).squeeze(-1)\n",
    "        # clear buffer\n",
    "        self.states, self.action_probs, self.rewards, self.dones, self.next_states = [], [], [], [], []\n",
    "\n",
    "        # TODO:\n",
    "        ########## Your code starts here. ##########\n",
    "        # Hints: 1. calculate the TD target as well as the MSE loss between the predicted value and the TD target\n",
    "        #        2. calculate the policy loss (similar to ex5) with advantage calculated from the value function. Normalise\n",
    "        #           the advantage to zero mean and unit variance.\n",
    "        #        3. update parameters of the policy and the value function jointly\n",
    "\n",
    "\n",
    "        # calculate the target values\n",
    "\n",
    "        # calculate the critic_loss\n",
    "\n",
    "\n",
    "        # Advantage estimation\n",
    "\n",
    "\n",
    "        # Compute the optimization term \n",
    "\n",
    "\n",
    "        # Compute the gradients of loss w.r.t. network parameters \n",
    "\n",
    "\n",
    "        # Update network parameters using self.optimizer and zero gradients \n",
    "\n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "\n",
    "        return {}\n",
    "\n",
    "\n",
    "    def get_action(self, observation, evaluation=False):\n",
    "        \"\"\"Return action (np.ndarray) and logprob (torch.Tensor) of this action.\"\"\"\n",
    "        if observation.ndim == 1: observation = observation[None] # add the batch dimension\n",
    "        x = torch.from_numpy(observation).float().to(device)\n",
    "\n",
    "        # TODO: Task 1\n",
    "        ########## Your code starts here. ##########\n",
    "        # Hints: 1. the self.policy returns a normal distribution, check the PyTorch document to see \n",
    "        #           how to calculate the log_prob of an action and how to sample.\n",
    "        #        2. if evaluating the policy, return policy mean, otherwise, return a sample\n",
    "        #        3. the returned action and the act_logprob should be torch.Tensors.\n",
    "        #            Please always make sure the shape of variables is as you expected.\n",
    "        \n",
    "        # calculate action\n",
    "        #action = \n",
    "\n",
    "        \n",
    "        # calculate the log probability of the action\n",
    "        #act_logprob = \n",
    "\n",
    "        ########## Your code ends here. ###########\n",
    "\n",
    "        return action, act_logprob\n",
    "\n",
    "\n",
    "    def record(self, observation, action_prob, next_observation, reward, done):\n",
    "        self.states.append(torch.tensor(observation, dtype=torch.float32))\n",
    "        self.action_probs.append(action_prob)\n",
    "        self.rewards.append(torch.tensor([reward], dtype=torch.float32))\n",
    "        self.dones.append(torch.tensor([done], dtype=torch.float32))\n",
    "        self.next_states.append(torch.tensor(next_observation, dtype=torch.float32))\n",
    "\n",
    "\n",
    "    def load(self, filepath):\n",
    "        d = torch.load(filepath)\n",
    "        self.policy.load_state_dict(d['policy'])\n",
    "        self.value.load_state_dict(d['value'])\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        torch.save({\n",
    "            'policy': self.policy.state_dict(),\n",
    "            'value': self.value.state_dict(),\n",
    "        }, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b02f2-891d-40c1-b01a-9bd370009a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init agent\n",
    "with open(Path().cwd()/'cfg'/'pg_ac.yaml', 'r') as f:\n",
    "    cfg = u.Struct(**yaml.safe_load(f))\n",
    "    \n",
    "agent = PG(cfg.state_shape[0], cfg.action_dim, cfg.lr, cfg.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad7389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.train(agent, cfg_path=Path().cwd()/'cfg'/'pg_ac.yaml', cfg_args={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8041f0c-73f8-4f23-bbd2-4a976a8b842a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.plot(cfg_path=Path().cwd()/'cfg'/'pg_ac.yaml',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4922b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.test(agent, cfg_path=Path().cwd()/'cfg'/'pg_ac.yaml', cfg_args=dict(save_video=True,testing=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb556b-187c-4662-a1df-60dc87917930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(Path().cwd()/'results'/'InvertedPendulum-v4'/'video'/'test'/'ex6-episode-9.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d432792",
   "metadata": {},
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.1</b> Relationship between actor-critic and REINFORCE with baseline (10 points) </h3> \n",
    "\n",
    "What is the relationship between actor-critic and REINFORCE with baseline?\n",
    "            \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837ef04",
   "metadata": {},
   "source": [
    "The relationship between the two is the estimation of return: actor-critic is nearly the same as REINFORCE with baseline, except that actor-critic uses the value function as the baseline. In REINFORCE, it uses Monte Carlo as the episodic return $Q(s_t, a_t)$ minus the baseline, while actor-critic uses the temporal difference (TD) error, which includes the immediate reward plus the estimated value of the next state minus the current state's value estimate. This difference is called the advantage function and we want to maximize it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88b08e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.2</b> Advantage (5 points) </h3> \n",
    "\n",
    "How can the value of advantage be intuitively interpreted?\n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3bac64",
   "metadata": {},
   "source": [
    "The advantage is the average difference in return if we select any other different action compared to the action dictated by the current policy. If the advantage is positive, it means the chosen action will yield higher returns and if the advantage is negative, the action will yield lower returns. Therefore, we need to maximize the advantage function to make better returns. A notable point is that an advantage function does not introduce a bias into the policy gradient estimate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db6c52",
   "metadata": {},
   "source": [
    "<a id='Q3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.3</b> Bias and Variance Analysis (10 points) </h3> \n",
    "\n",
    "How does the implemented actor-critic method compare to REINFORCE in terms of bias and variance of the policy gradient estimation? Explain your answer.    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106d5916",
   "metadata": {},
   "source": [
    "__1. Bias comparison between two models__\n",
    "\n",
    "- Actor-critic model: they often use bootstrapping, where the value function by the critic is used to estimate the return. This will introduce an inherent bias at the beginning. As training progresses and the value function becomes more accurate, the bias tends to decrease and becomes less of an issue.\n",
    "\n",
    "- REINFORCE: it uses Monte Carlo estimation for the return without using any bootstrapping. This means that REINFORCE does not introduce bias like the actor-critic model.\n",
    "\n",
    "$\\implies$ Comparison: REINFORCE generally has lower bias compared to actor-critic methods in the early stages of training. But after training for a while, the difference in bias between the two becomes negligible.\n",
    "\n",
    "__2. Variance comparison between two models__\n",
    "\n",
    "- Actor-critic: by using the value function (critic) to estimate the return, actor-critic methods often reduce the variance of the policy gradient estimate. The critic provides a more stable estimate compared to the potentially noisy Monte Carlo samples. Additionally, actor-critic updates the policy at each time step, allowing for more frequent updates and potentially reducing variance. Therefore, actor-critic becomes sampling efficient. \n",
    "\n",
    "- REINFORCE: it relies on Monte Carlo methods, which can lead to high variance in the policy gradient estimation. The returns are computed based on the sampled trajectories, which can vary significantly. Because REINFORCE requires the completion of an entire episode, the expected returns can have higher variance, especially in stochastic environments.\n",
    "\n",
    "$\\implies$ Comparison: REINFORCE generally has higher variance compared to actor-critic methods due to using Monte Carlo method instead of temporal difference method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ddf4e7",
   "metadata": {},
   "source": [
    "<a id='Q4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.4</b> Controlling bias-variance tradeoff (10 points) </h3> \n",
    "\n",
    "How could the bias-variance tradeoff in actor-critic be controlled?\n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03501249",
   "metadata": {},
   "source": [
    "We can control the bias-variance trade-off in actor-critic by using different advantage functions or bootstrapping methods. \n",
    "\n",
    "- The advantage function represents the relative value of taking a specific action compared to the average value of a specific state. The advantage function used in this exercise, for example, encourage lower variance but higher bias at the begin of training for the actor-critic model for using the baseline. Without the baseline, the variance can become higher. \n",
    "\n",
    "- Bootstrapping is using value function estimates to approximate future returns. Different bootstrapping methods can have effects on bias and variance. For Monte Carlo, there is no bootstrapping and thus, no bias is introduced and the variance becomes high as the model uses episodic returns. On the other hand, temporal difference bootstrapping will introduce bias by using immediate value function estimates, but it also significantly reduces variance by relying less on episodic returns. We can also use TD-Lambda for intermediate bootstrapping to achieve varying levels of bias and variance in actor-critic models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1e91e-5715-45b9-a2c2-4e8f0fd2fff2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Submitting <a id='3.'></a>\n",
    "Ensure all tasks and questions (in ```ex6_DDPG.ipynb``` and ```ex6_PG_AC.ipynb```) are answered and the necessary plots are saved in the appropriate locations. The relevant plots and files needed to be submitted for this assignment are:\n",
    "\n",
    "\n",
    "- Training performance plots:\n",
    "  - `pg_ac.png`: Training performance plots in terms of episode and episodic reward\n",
    "<br>\n",
    "\n",
    "  \n",
    "- Model files:\n",
    "  - `InvertedPendulum-v4_params.pt`: Trained model\n",
    "\n",
    "\n",
    "Ensure the model files and plots are saved in correct paths:\n",
    "- ```results/InvertedPendulum-v4/pg_ac.png``` Training result\n",
    "- ```results/InvertedPendulum-v4/model/InvertedPendulum-v4_params.pt``` Training Model\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER TASK IN ```ex6_DDPG.ipynb```** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0c6d7-563a-4037-9d4c-ef1422d7b657",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1 Feedback <a id='3.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer to the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e7c42-211b-419b-a3c9-daeab45597f1",
   "metadata": {},
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa3e0a-f247-43f4-92cc-5b6e99c79049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hrs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e49e4f-0803-4d11-9ac4-9f77bedfa52f",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc5ed4-1957-4c5c-97fb-de2dc7c4362e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = 5   # Implementing DDPG (20 points)\n",
    "Q1 = 5   # Question 1.1 Relationship between actor-critic and REINFORCE with baseline (10 points)\n",
    "Q2 = 5   # Question 1.2 Advantage (5 points)\n",
    "Q3 = 5   # Question 1.3 Bias and Variance Analysis (10 points)\n",
    "Q4 = 5   # Question 1.4 Controlling bias-variance tradeoff (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6cdd1-ea58-4a06-a81b-1d7cc36f14a2",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27706e2f-2a35-4a6e-ad07-9af944b96dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = 5   # Implementing DDPG (20 points)\n",
    "Q1 = 5   # Question 1.1 Relationship between actor-critic and REINFORCE with baseline (10 points)\n",
    "Q2 = 5   # Question 1.2 Advantage (5 points)\n",
    "Q3 = 5   # Question 1.3 Bias and Variance Analysis (10 points)\n",
    "Q4 = 5   # Question 1.4 Controlling bias-variance tradeoff (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc810cc9-ba38-480d-a95e-178f875bc744",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "Please share any additional feedback, suggestions, or comments you have about the lecture, assignment, or course content. Your input is valuable in helping us improve the learning experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6d30ad-8d44-4fe5-a642-121200f920d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785254f3-48b6-44a6-ace6-0eae136d6396",
   "metadata": {},
   "source": [
    "# References <a id='4.'></a>\n",
    "Please use the following section to record references."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

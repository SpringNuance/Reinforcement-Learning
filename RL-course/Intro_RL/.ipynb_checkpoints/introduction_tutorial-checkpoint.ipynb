{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ll4Cf0c5S44"
   },
   "source": [
    "# Preliminary to reinforcement learning course (Aalto University)\n",
    "## Gentle introduction to Numpy, Pytorch, wandb, Markdown, and LaTeX\n",
    "\n",
    "Inspired from below reference:\n",
    "* [CS 285 at UC Berkeley](https://rail.eecs.berkeley.edu/deeprlcourse/)\n",
    "* [CS 224R at Stanford](https://cs224r.stanford.edu/)\n",
    "* [Pytorch Tutorials](https://pytorch.org/tutorials/)\n",
    "* [Numpy Tutorials](https://numpy.org/doc/stable/user/absolute_beginners.html)\n",
    "* Also special thanks to **Yi Zhao** for providing previous version of this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xR-ZrEQb5S47"
   },
   "source": [
    "## Part 1: Numpy\n",
    "Numpy arrays are objects that allow you to store and manipulate matrices.\n",
    "\n",
    "Numpy is *much* faster than pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCQYfXSw5S47"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pUSVA8B5S48"
   },
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4, 5, 6])\n",
    "y = np.array([[8, 0, 7], [3, 0, 1]])\n",
    "z = np.random.rand(3, 2, 3) #uniform distribution\n",
    "\n",
    "print(f\"x, shape={x.shape}:\\n{x}\\n\")\n",
    "print(f\"y, shape={y.shape}:\\n{y}\\n\")\n",
    "print(f\"z, shape={z.shape}:\\n{z}\\n\")\n",
    "\n",
    "def benchmark_python_add(list_a: list, list_b: list):\n",
    "    result = []\n",
    "    for a, b in zip(list_a, list_b):\n",
    "      result.append(a + b)\n",
    "    return result\n",
    "\n",
    "def benchmark_python_add_expression(list_a: list, list_b: list):\n",
    "    result = [a+b for a, b in zip(list_a, list_b)]\n",
    "    return result\n",
    "\n",
    "def benchmark_numpy_add(array_a: np.ndarray, array_b: np.ndarray):\n",
    "    return array_a + array_b\n",
    "\n",
    "arr_a = np.random.normal(size=(100000,))\n",
    "list_a = list(arr_a)\n",
    "arr_b = np.random.normal(size=(100000,))\n",
    "list_b = list(arr_b)\n",
    "print(\"Python:\")\n",
    "%timeit benchmark_python_add(list_a, list_b)\n",
    "print(\"Python Expression:\")\n",
    "%timeit benchmark_python_add_expression(list_a, list_b)\n",
    "print(\"Numpy:\")\n",
    "%timeit benchmark_numpy_add(arr_a, arr_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKDIV-Rq5S48"
   },
   "source": [
    "### Numpy arithmetic and broadcasting\n",
    "You can add/subtract/multiple/divide numpy arrays, as long as their dimensions match.\n",
    "\n",
    "You can also sometimes do arithmetic operations on arrays whose dimensions don't match exactly. Whenever possible, values will be \"copied\" so that the dimensions match. This is called [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html), and it has some benefits:\n",
    "* More concise code\n",
    "* Duplicated values aren't explicitly created/stored in memory\n",
    "* Repeated operations are optimized to run faster\n",
    "\n",
    "But at the same time, you need to be careful not to misuse it and create bugs that cannot be easily traced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twKamck65S49"
   },
   "outputs": [],
   "source": [
    "a = np.ones((2, 3))\n",
    "y = np.array([[8, 0, 7], [3, 0, 1]])\n",
    "\n",
    "print(a)\n",
    "print(\"+\")\n",
    "print(y)\n",
    "print(\"=\")\n",
    "print(a + y)\n",
    "\n",
    "print(f\"3\\n*\\n{a}\\n=\\n{3*a}\\n\")\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "print(f\"{a}\\n+\\n{y}\\n=\\n{a+y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQNPdV1s5S49"
   },
   "source": [
    "### Axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKG5F2Is5S49"
   },
   "source": [
    "Numpy arrays have **axes**, which are like the \"directions\" along which you can do things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VM3YmxgE5S4-"
   },
   "outputs": [],
   "source": [
    "x = np.array([[8, 7, 4], [5, 2, 2], [1, 6, 3]])\n",
    "print('tensor:')\n",
    "print(x)\n",
    "print()\n",
    "print('sum operations: ')\n",
    "print(np.sum(x))\n",
    "print(np.sum(x, axis=0))\n",
    "print(np.sum(x, axis=1))\n",
    "print()\n",
    "print('max operations: ')\n",
    "\n",
    "print(np.max(x))\n",
    "print(np.max(x, axis=0))\n",
    "print(np.max(x, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HsmwBBd5S4-"
   },
   "source": [
    "More generally, you can think of an axis as the index of one of the values in the array's `shape`. If you do a reduction operation (e.g. sum, max) on a certain axis, that axis will disappear from the array's shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3772bdL5S4-"
   },
   "outputs": [],
   "source": [
    "z = np.random.randint(10, size=(3, 2, 4, 5))\n",
    "print(np.sum(z, axis=2).shape)\n",
    "print(np.sum(z, axis=(0, 2)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bk_SfQw5S4-"
   },
   "source": [
    "### Shapes and reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRY7sqiK5S4-"
   },
   "source": [
    "Be careful about the shape of your numpy arrays, especially when you're working with vectors (where one of the dimensions is 1)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltPJVogA5S4-"
   },
   "outputs": [],
   "source": [
    "# These two are different things!\n",
    "x = np.random.randint(10, size=(10,))\n",
    "y = np.random.randint(10, size=(10, 1))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcuHY0gR5S4-"
   },
   "outputs": [],
   "source": [
    "# If you multiply a shape (n,) array with a (n,1) array, you actually get a shape (n,n) matrix:\n",
    "print(x)\n",
    "print(\"*\")\n",
    "print(y)\n",
    "print(\"=\")\n",
    "print(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwrrepXc5S4_"
   },
   "source": [
    "When writing numpy code, you should **always** keep track of the expected shape of each array and what each axis corresponds to. There are a few ways to do this:\n",
    " * Asserts (`assert x.shape == (batch_size, hidden_dim)`)\n",
    " * Comments (`# shape (B, C, W, H)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J66HoEXh5S4_"
   },
   "outputs": [],
   "source": [
    "z = x * y\n",
    "assert z.shape == x.shape, f\"Expected x*y to have shape {x.shape} but got {z.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoG_CEy35S4_"
   },
   "outputs": [],
   "source": [
    "# If your intention is to multiply the two vectors element-wise, you need to reshape one of them first!\n",
    "x_reshaped = x.reshape(-1, 1) # -1 means \"infer this dimension\"\n",
    "print()\n",
    "print(x_reshaped)\n",
    "print(\"*\")\n",
    "print(y)\n",
    "print(\"=\")\n",
    "print(x_reshaped * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpwQVDQO5S4_"
   },
   "source": [
    "### Array Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgqe-B4w5S4_"
   },
   "source": [
    "Numpy has two multiplication operators: `*` and `@`.\n",
    "- `*` does **element-wise** multiplication.\n",
    "- `@` does **matrix** multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3UOiMdW5S4_"
   },
   "outputs": [],
   "source": [
    "matrix = np.random.randint(10, size=(5, 5))\n",
    "row_vec = np.random.randint(10, size=(5,))\n",
    "col_vec = row_vec.reshape(5, 1)\n",
    "\n",
    "# Outer product\n",
    "print(col_vec)\n",
    "print(\"*\")\n",
    "print(row_vec)\n",
    "print(\"=\")\n",
    "print(col_vec * row_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwN5_kLM5S4_"
   },
   "outputs": [],
   "source": [
    "# Dot product\n",
    "print(row_vec)\n",
    "print(\"@\")\n",
    "print(col_vec)\n",
    "print(\"=\")\n",
    "# print((row_vec @ col_vec)) # Result will be a shape (1,) array\n",
    "# print(row_vec.dot(col_vec).squeeze()) # Same as above\n",
    "print(row_vec @ col_vec.squeeze()) # Works on arrays with same shape too (to get scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjRJq5k65S4_"
   },
   "source": [
    "Below are some other common Numpy operations you'll probably find useful at some point in this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGamS7_L5S4_"
   },
   "outputs": [],
   "source": [
    "# transpose: reorders the axes of the array\n",
    "z = np.random.rand(28, 28, 3)\n",
    "z_transposed = z.transpose((2, 0, 1))\n",
    "assert z_transposed.shape == (3, 28, 28)\n",
    "print(\"Orig shape:\", z.shape)\n",
    "print(\"New shape: \", z_transposed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1N5dCy45S4_"
   },
   "source": [
    "### Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIrJotqM5S4_"
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(2,2) # uniform sampling, arguments are dimensions\n",
    "print(x)\n",
    "\n",
    "x = np.random.rand(2,2) # normal distribution, arguments are dimensions\n",
    "print()\n",
    "print(x)\n",
    "\n",
    "x = 5+0.5*np.random.randn() # Gaussian distribution with 5 as mean and 0.5 as std\n",
    "print()\n",
    "print(x)\n",
    "\n",
    "x = np.random.randint(0, 10, size=(5,1)) # unifrom integers between low and high, also can determine size\n",
    "print()\n",
    "print(x)\n",
    "\n",
    "x = np.arange(0,10,2) # similar to python range, but create numpy array\n",
    "print('Before shuffling')\n",
    "print(x)\n",
    "np.random.shuffle(x)\n",
    "print('After shuffling')\n",
    "print(x)\n",
    "\n",
    "print()\n",
    "print(np.random.choice(x, 2)) # randomly select k element from an array\n",
    "\n",
    "# multivariate gaussian\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0], [0, 100]]  # diagonal covariance\n",
    "x = np.random.multivariate_normal(mean, cov, (3, 3))\n",
    "print()\n",
    "print(x, x.shape) # notice the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhvrz9Gs5S4_"
   },
   "source": [
    "## Part2: Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xEJDTWY5S4_"
   },
   "source": [
    "* Similar to Numpy, allow fast tensor operation, but also supports GPU computation\n",
    "* Support autodiff by computation graph\n",
    "* Different neural network architectures and operations\n",
    "* Build-in optimizer and tools for datasets and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WD-oYQVq5S4_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.zeros(2, 3)\n",
    "y = torch.ones(2, 3)\n",
    "z = x + y\n",
    "\n",
    "print(x)\n",
    "print(\"+\")\n",
    "print(y)\n",
    "print(\"=\")\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-AEdBC45S4_"
   },
   "source": [
    "Reduction operations work the same way as they do in Numpy, except we use the argument `dim` instead of `axis`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enbfAm3f5S4_"
   },
   "outputs": [],
   "source": [
    "print(torch.sum(z, dim=1))\n",
    "print(torch.sum(z, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qN1S2LIv5S4_"
   },
   "source": [
    "Also like Numpy, PyTorch will try to broadcast operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OLJZM9n5S5A"
   },
   "outputs": [],
   "source": [
    "x = torch.ones((3, 1))\n",
    "y = torch.ones((1, 3))\n",
    "z = x + y\n",
    "\n",
    "print(x)\n",
    "print(\"+\")\n",
    "print(y)\n",
    "print(\"=\")\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0SftU2X5S5A"
   },
   "source": [
    "### Moving between numpy and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hN7pHAhv5S5A"
   },
   "outputs": [],
   "source": [
    "x_np = np.random.randn(2, 3)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDzRsjx75S5A"
   },
   "source": [
    "Use `torch.from_numpy` to convert from numpy array -> PyTorch tensor. The resulting tensor shares the same memory as the numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyaD40zx5S5A"
   },
   "outputs": [],
   "source": [
    "x = torch.from_numpy(x_np)\n",
    "print(x)\n",
    "\n",
    "x_np[:] = 0\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ijw5U4-G5S5A"
   },
   "source": [
    "By default, numpy arrays are float64. You'll probably want to convert arrays to float32, as most tensors in pytorch are float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-iLZbvDt5S5A"
   },
   "outputs": [],
   "source": [
    "x = torch.from_numpy(x_np).float()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbF1DW5v5S5E"
   },
   "source": [
    "Use `.numpy()` to convert from PyTorch tensor -> numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtLpukP_5S5E"
   },
   "outputs": [],
   "source": [
    "print(x.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OT2_xeyz5S5E"
   },
   "source": [
    "### Let's start with linear regression\n",
    "Suppose we have a bunch of data points generated from a linear model $y_i = wx_i + b$ with additive noise. Our task is to decide the linear model's weight $w$ and bias $b$ using these data on hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuOMa2lW5S5E"
   },
   "outputs": [],
   "source": [
    "# generate synthetic data\n",
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.5, y.shape)  # additive noise\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([-3.4])\n",
    "true_b = torch.tensor([4.2])\n",
    "\n",
    "# generate data\n",
    "features, labels = synthetic_data(true_w, true_b, 1000) # generate 1000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qG6hakKC5S5E"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot data\n",
    "plt.scatter(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wp7FTres5S5E"
   },
   "outputs": [],
   "source": [
    "# process the training data\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = np.arange(num_examples)\n",
    "    # The examples are read at random, in no particular order\n",
    "    np.random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(indices[i:min(i +\n",
    "                                                   batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQyqXARI5S5E"
   },
   "source": [
    "### Initialize model parameters and define the model\n",
    "To learn a model from data points, let's first define the model form as $y = wx + b$ and initialize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Cj8Py4s5S5E"
   },
   "outputs": [],
   "source": [
    "w = torch.normal(mean=torch.tensor([0.0]), std=torch.tensor([0.1]))\n",
    "b = torch.zeros(1)\n",
    "print(f'The initial weight is: {w}, the intial bias is {b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjHti2LL5S5E"
   },
   "source": [
    "However, this is not enough! If we want to calculate the gradient w.r.t $w$ and $b$, we have to define it explicitly by setting `requires_grad=True`. This is important, otherwise, we can't optimize these parameters by the optimizer. In practice, you should always take care of which parameter should be set `requires_grad` as `True` and when to set them. Also, in some cases, we need to set `requires_grad` as `False`, and we will discuss it in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1gAGXzx5S5E"
   },
   "outputs": [],
   "source": [
    "w.requires_grad = True\n",
    "b.requires_grad = True\n",
    "print(f'After set requires_grad=Ture, the w is: {w}, and b is {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BF_XbM7g5S5E"
   },
   "outputs": [],
   "source": [
    "def model(x, w, b):\n",
    "    \"\"\" Define the linear regression model.\"\"\"\n",
    "    return w * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocpnF37L5S5E"
   },
   "source": [
    "### Define the loss function and define the optimization algorithm\n",
    "The optimization objective we used here is the **mean square error**: $L = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{2}(y_i - \\hat{y_i})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Fsmsq4B5S5F"
   },
   "outputs": [],
   "source": [
    "def loss_fn(y_hat, y):\n",
    "    return (y - y_hat)**2 * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95KFFTDu5S5F"
   },
   "source": [
    "Once we define the model and the loss function, there are many methods to define the parameters $w$ and $b$ to minimize the loss function. For example, we can calculate the optimal parameters analytically, or we perform the optimization with gradient descent. This tutorial will use the second one.\n",
    "\n",
    "In order to use the gradient descent method, we need to first calculate the gradient as:\n",
    "\n",
    "$$\\frac{dL}{dw} = \\frac{1}{N}\\sum_{i=1}^N -x_i(y_i - w x_i - b) = \\frac{1}{N}\\sum_{i=1}^N(w x_i^2 + b x_i - x_i y_i)$$\n",
    "\n",
    "$$\\frac{dL}{db} = \\frac{1}{N} \\sum_{i=1}^N -(y_i - w x_i - b) = \\frac{1}{N} \\sum_{i=1}^N (w x_i + b - y_i)$$\n",
    "\n",
    "With the gradient w.r.t $w$ and $b$, we finally perform the gradient descent step:\n",
    "\n",
    "$$ w \\leftarrow w - \\alpha \\frac{dL}{dw}$$\n",
    "\n",
    "$$ b \\leftarrow b - \\alpha \\frac{dL}{db}$$\n",
    "\n",
    "Notice that the gradient is estimated using the whole training set. This is fine when the training set is small, but when the training set is large, e.g., ImageNet, calculating over the whole training set to perform one gradient step is expensive. Thus, the simple yet efficient solution is estimating the gradient with a mini batch of training data, we call this method as stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qWBzWy75S5F"
   },
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    with torch.no_grad(): # freeze params for saving resources\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()  # reset the gradient as 0 !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4obGica5S5F"
   },
   "source": [
    "### Training\n",
    "Now, we can finally perform the training process. But there is still something we should take care of, that is the hyperparameters. Specifically, in our toy linear regression example, we should define the learning rate $\\alpha$ and *batch size*. Hyperparameters are important for machine learning since they hugely influence the training results. So you should always tune them carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTmej8GB5S5F"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGm7KrzK5S5F"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "for epoch in range(num_epoch):\n",
    "    for x, y in data_iter(batch_size, features, labels):\n",
    "        y_hat = model(x, w, b)\n",
    "        loss = loss_fn(y_hat, y).sum()\n",
    "        # compute gradient on loss w.r.t w and b\n",
    "        loss.backward()\n",
    "        sgd([w, b], learning_rate, batch_size)\n",
    "\n",
    "    # print\n",
    "    with torch.no_grad():\n",
    "        train_loss = loss_fn(model(features, w, b), labels)\n",
    "        print(f'epoch {epoch+1}, loss {float(train_loss.mean())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0M3g8pK5S5F"
   },
   "source": [
    "Notice that instead of calcuating the gradient w.r.t $w$ and $b$, we directly call the function `.backward()` on loss. The function is to calculate the gradient w.r.t parameters with `requires_grad=True`. The resulting gradient for each parameter is stored in `param.grad`, see our `sgd()` function. For more detail, please check the document https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0k9YhEz5S5F"
   },
   "outputs": [],
   "source": [
    "# compare w and b with the ground truth\n",
    "print(f'The estimated w is {w.item()}, the true w is {true_w.item()};')\n",
    "print(f'The estimated b is {b.item()}, the true b is {true_b.item()}')\n",
    "\n",
    "# plot it\n",
    "plt.scatter(features, labels)\n",
    "x = range(-4, 5)\n",
    "y = [model(i,w,b).detach().numpy() for i in x]\n",
    "plt.plot(x, y, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpv-GY9v5S5F"
   },
   "source": [
    "### Change different optimizers\n",
    "In the above example, we define the `sgd()` optimizer by ourselves. However, Pytorch already offers a lot of different optimizers, see https://pytorch.org/docs/stable/optim.html#how-to-use-an-optimizer. Here, we will show how to use the `Adam` optimizer offered by Pytorch. We will show it by keeping the model the same, but change the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbICbfKe5S5F"
   },
   "outputs": [],
   "source": [
    "# let's first define the adam optimizer\n",
    "optimizer = torch.optim.Adam([w, b], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOJko7xl5S5F"
   },
   "source": [
    "Again, let's first initialize the parameters. Since we already define the $w$ and $b$, we can easily modify their data by changing `w.data` and `b.data`. Notice that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQalw-YI5S5F"
   },
   "outputs": [],
   "source": [
    "# Because we want to show\n",
    "w.data = torch.normal(mean=torch.tensor([0.]), std=torch.tensor([0.1]))\n",
    "b.data = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0orl0C445S5F"
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "for epoch in range(num_epoch):\n",
    "    for x, y in data_iter(batch_size, features, labels):\n",
    "        optimizer.zero_grad() # ! Remember to call zero_grad()\n",
    "        y_hat = model(x, w, b)\n",
    "        loss = loss_fn(y_hat, y).sum()\n",
    "\n",
    "        # calculat gradient on loss w.r.t w and b\n",
    "        loss.backward()\n",
    "        # preform the gradient descent step to update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # print\n",
    "    with torch.no_grad():\n",
    "        train_loss = loss_fn(model(features, w, b), labels)\n",
    "        print(f'epoch {epoch+1}, loss {float(train_loss.mean())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tti1npx-5S5F"
   },
   "source": [
    "Here, we want to stress the `optimizer.zero_grad()` function. The `backward()` function, by default, accumulates gradients in the `.grad` buffer. This means that every call to `backward` **adds the gradient** to what is currently stored in the buffer, instead of overwriting it. This is useful when dealing with large models and dataset/batch sizes, where the whole data doesn't fit into memory and the gradients have to be calculated for a larger number of samples and averaged. In our case, we want to calcualte the current gradient value at each iteration, so we have to manually zero out the gradients by calling `optimizer.zero_grad()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBVCnBPJ5S5F"
   },
   "outputs": [],
   "source": [
    "# compare w and b with the ground truth\n",
    "print(f'The estimated w is {w.item()}, the true w is {true_w.item()};')\n",
    "print(f'The esimated b is {b.item()}, the true b is {true_b.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-INR8Fp5S5F"
   },
   "source": [
    "## 2. Basic Neural Network\n",
    "In our assignments, we will use very basic neural networks, especially multilayer perceptrons (MLPs). \"For certain choices of the activation function, it is widely known that MLPs are **universal approximators**. Even with a single-hidden-layer network, given enough nodes (possibly absurdly many), and the right set of weights, **we can model any function**, though actually learning that function is the hard part.\" MLPs are vastly adopted in reinforcement learning to represent the policy, the value function or the dynamic model. In this tutorial, we will quickly go through how to define MLPs and how to train it. For more detail, please check http://www.d2l.ai/chapter_multilayer-perceptrons/mlp.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4dT6Ris5S5G"
   },
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvYo0AQU5S5G"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "net = Net(1, 1, 10)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iV8GiQmV5S5G"
   },
   "source": [
    "The above `Net` class defines a simple two-layer neural network. In `__init__` function, you can initialize the network, and the operations on inputs are implemented in the `forward` function. As a subclass of `nn.Module`, the backpropagation is automatically built after implementing the `forward` function.\n",
    "\n",
    "`nn.Linear()` is used to define the linear layer $y = w x + b$, and `F.relu()` is the activition function, defined as $relu(x) = max(x, 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtTnUg2Z5S5G"
   },
   "source": [
    "### Train the network\n",
    "To be concise, we will use the defined MLPs to approximate a linear model similar to our linear regression example. Please remember that the MLPs is a powerful function approximator, so they can represent much more complex models. We will use the neural network to solve a more complex task later.\n",
    "\n",
    "In this toy example, we will use the same loss function, the same dataset as above, the only difference is instead of using the linear model, we use two-layer MLPs here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2CPeb3v5S5G"
   },
   "source": [
    "The parameters of the MLPs are automatically initialized when defining them. The default initialized values depend on the type of layers. Specifically, for linear layers, the initialized value is given by $U(-\\sqrt{k}, \\sqrt{k})$, where $k = \\frac{1}{in\\_features}$ (see https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py#L435-L68).\n",
    "\n",
    "The initialized values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8ZvQtBr5S5G"
   },
   "outputs": [],
   "source": [
    "for w in net.parameters():\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFTSdX-G5S5G"
   },
   "source": [
    "Now, let's train our network similar to the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8OTAEqc5S5G"
   },
   "outputs": [],
   "source": [
    "# Define the optimizer,\n",
    "num_epoch=15\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "# training loop\n",
    "for epoch in range(num_epoch):\n",
    "    for x, y in data_iter(batch_size, features, labels):\n",
    "        optimizer.zero_grad() # ! Remember to call zero_grad()\n",
    "        y_hat = net(x)\n",
    "        loss = loss_fn(y_hat, y).sum()\n",
    "\n",
    "        # calculat gradient on loss w.r.t w and b\n",
    "        loss.backward()\n",
    "        # preform the gradient descent step to update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # print\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        train_loss = loss_fn(net(features), labels)\n",
    "        print(f'epoch {epoch+1}, loss {float(train_loss.mean())}')\n",
    "        net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXGEjJE25S5G"
   },
   "outputs": [],
   "source": [
    "# plot it\n",
    "plt.scatter(features, labels)\n",
    "x = torch.tensor(range(-4, 5), dtype=torch.float32).unsqueeze(-1)\n",
    "y = [net(i).detach().numpy() for i in x]\n",
    "plt.plot(x, y, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTh4Rah95S5G"
   },
   "source": [
    "#### The `detach()` operator\n",
    "It might be more straightforward to understand the `detach()` operator by showing how it influences the `w` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlfZ4cVk5S5G"
   },
   "outputs": [],
   "source": [
    "print(f'Before calling detach(), w is {w}')\n",
    "print(f'After calling detach(), w is {w.detach()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8tIfYIo5S5G"
   },
   "source": [
    "Obviously, by calling the `detach()` on the parameter `w`, the returned new Tensor doesn't require grad anymore, such its value won't be updated by the optimizer.\n",
    "\n",
    "Notice that the `w.detach()` returns a new Tensor, but the original `w` is not modified, which means the `w` still requires grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTtecvgu5S5G"
   },
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqmO11C85S5G"
   },
   "source": [
    "### Distributions\n",
    "PyTorch has a very convenient [distributions](https://pytorch.org/docs/stable/distributions.html) package for probability distributions. This is useful since in RL we often work with stochastic policies (e.g. $\\pi(a|s_0)= $ some Gaussian distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZT6B6XVU5S5G"
   },
   "outputs": [],
   "source": [
    "from torch import distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LXPKYZR5S5G"
   },
   "source": [
    "You create distributions by passing the parameters of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjONCqIN5S5G"
   },
   "outputs": [],
   "source": [
    "# Univariate Gaussian with mean 0, std 1\n",
    "mean = torch.zeros(1, requires_grad=True)\n",
    "std = torch.ones(1, requires_grad=True)\n",
    "gaussian = distributions.Normal(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dp5pWU3L5S5G"
   },
   "source": [
    "These distributions are instances of the more general `Distribution` class, which you can read more about [here](https://pytorch.org/docs/stable/distributions.html#distribution).\n",
    "\n",
    "The two most useful operations you can do with `Distribution` objects are `sample` and `log_prob`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRkc-I6k5S5H"
   },
   "outputs": [],
   "source": [
    "sample = gaussian.sample((1,))\n",
    "print(sample)\n",
    "sample_grad = gaussian.rsample((1,))\n",
    "print(sample_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvsNbgNI5S5H"
   },
   "outputs": [],
   "source": [
    "log_prob = gaussian.log_prob(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X39JfI9m5S5H"
   },
   "source": [
    "The log probability depends on the the parameters of the distribution. So, calling `backward` on a loss that depends on `log_prob` will back-propagate gradients into the parameters of the distribution.\n",
    "\n",
    "NOTE: this won't back-propagate through the samples (the \"reparameterization trick''), unless you use `rsample`, which is only implemented for some distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VGDxNxh5S5H"
   },
   "source": [
    "$$s_1, a_1, s_2, a_2, ...$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e68z4JD5S5H"
   },
   "outputs": [],
   "source": [
    "loss = -log_prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUc6iKLj5S5H"
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvRNbMzD5S5H"
   },
   "outputs": [],
   "source": [
    "print(mean.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxNVAW3G5S5H"
   },
   "source": [
    "### Batch-Wise distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQUdzPSA5S5H"
   },
   "source": [
    "The distributions also support batch-operations. In this case, all the operations (`sample`, `log_prob`, etc.) are batch-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2i4yfws5S5H"
   },
   "outputs": [],
   "source": [
    "mean = torch.zeros(10)\n",
    "std = torch.ones(10)\n",
    "gaussian = distributions.Normal(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRWk3auU5S5H"
   },
   "outputs": [],
   "source": [
    "print(gaussian)\n",
    "print(gaussian.batch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fr4jL_Fk5S5H"
   },
   "outputs": [],
   "source": [
    "sample = gaussian.sample((3,))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNrji_Iw5S5H"
   },
   "outputs": [],
   "source": [
    "gaussian.log_prob(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7UkgzFP5S5H"
   },
   "source": [
    "### Multivariate Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFPmVK4u5S5H"
   },
   "source": [
    "You can also define a multivariate normal distribution, which allows you to use nondiagonal covariance matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBjXci3_5S5H"
   },
   "outputs": [],
   "source": [
    "mean = torch.zeros(2)\n",
    "covariance = torch.tensor(\n",
    "    [[1, 0.8],\n",
    "     [0.8, 1]]\n",
    ")\n",
    "gaussian = distributions.MultivariateNormal(mean, covariance) # can model dependencies between different components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAXNEHya5S5H"
   },
   "outputs": [],
   "source": [
    "gaussian.sample((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqI41pH45S5H"
   },
   "outputs": [],
   "source": [
    "samples = gaussian.sample((500,))\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.scatter(samples[:, 0].numpy(), samples[:, 1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izQAKONq5S5I"
   },
   "source": [
    "## Part3: wandb\n",
    "1. Create an account\n",
    "2. Login to the account (put authorize key)\n",
    "3. Use it to log information\n",
    "4. Exporting plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYYfo0ob5ckL"
   },
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pec_Wfh05S5I"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJmCtaWC5S5I"
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "        project='Aalto-Demonstration',\n",
    "        group='Ex1-Demonstration',\n",
    "        name='my-first try',\n",
    "        config={}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNZu7Hur5S5I"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2*np.pi)\n",
    "y1 = np.cos(x)\n",
    "y2 = np.sin(x)\n",
    "\n",
    "for t in range(len(x)):\n",
    "    wandb.log({\n",
    "        'x': x[t],\n",
    "        'cos': y1[t],\n",
    "        'sin': y2[t]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwyM4oNE5S5I"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

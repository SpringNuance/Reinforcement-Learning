{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53274fb5",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 1 - The Reinforcement Learning Framework</b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Sep 4, 2023 - Nov 30, 2023</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Learning Objectives </a>\n",
    "* <a href='#1.2'> 1.2 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Cartpole</a>\n",
    "* <a href='#3.'> 3. Reacher</a>\n",
    "* <a href='#4.'> 4. Submitting </a>\n",
    "* <a href='#4.1'> 4.1 Feedback </a>\n",
    "* <a href='#5.'> References</a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Training a Model for Simple Cartpole Environment (10 points)</a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> Learning (10 points)</a>\\\n",
    "<a href='#T2'><b>Student Task 2.</b> Investigating Training Performance (10 points) </a>\\\n",
    "<a href='#Q2'><b>Student Question 2.1</b> Analysis of Training Performance (15 points) </a>\\\n",
    "<a href='#Q3'><b>Student Question 2.2</b> Stochasticity (10 points) </a>\\\n",
    "<a href='#T3'><b>Student Task 3.</b> Reward Functions (20 points) </a>\\\n",
    "<a href='#T4'><b>Student Task 4.</b> Visualizing Behavior (10 points) </a>\\\n",
    "<a href='#Q4'><b>Student Question 4.1</b> Achieved Peformance (5 points)\\\n",
    "<a href='#Q5'><b>Student Question 4.2</b> Analysis of Behaviour (10 points)</a>\n",
    "    \n",
    "**Total Points:** 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab28e9",
   "metadata": {},
   "source": [
    "# 1. Introduction <a id='1.'></a>\n",
    "In this exercise we will take a first look at a reinforcement learning environment, its components and modify the reward function of a simple agent.\n",
    "\n",
    "In this notebook two environments are used: Cartpole and Reacher. The cartpole environment is taken from [OpenAI's Gym library](https://www.gymlibrary.dev/). The reacher environment is custom made (and defined in ```reacher.py```) but utilizes the Gym API.\n",
    "\n",
    "## 1.1 Learning Objectives: <a id='1.1'></a>\n",
    "- To become familiar with assignment structure and the agent-environment relationshp\n",
    "- To understand the effects of stochasticity\n",
    "- To understand and explore the effects of task definition\n",
    "\n",
    "## 1.2 Code Structure & Files <a id='1.2'></a>\n",
    "\n",
    "The ```train.py``` file instantiates the environment and the RL agent that acts in it. The ```agent.py``` file contains the implementation of a simple reinforcement learning agent; for the sake of this exercise, you can assume it to be a black box (you don’t need to understand how it works, although you are encouraged to study it in more detail). You don’t have to edit any other file other than ```ex1.ipynb``` to complete this exercise.\n",
    "```\n",
    "├───cfg                  # Config files for environments e.g. define the maximum number of steps in an episode.\n",
    "├───imgs                 # Images used in notebook\n",
    "├───results \n",
    "│   ├───logging          # Contains logged data\n",
    "│   ├───model            # Contains the policies learned\n",
    "│   └───video            # Contains videos for each environment\n",
    "│       └───CartPole-v0\n",
    "│       │  └───test      # Videos saved during testing\n",
    "│       │  └───train     # Videos saved during training\n",
    "│       └───SpinningReacher-v0\n",
    "│           └───test\n",
    "│           └───train\n",
    "│   ex1.ipynb            # Main assignment file containing tasks <---------\n",
    "│   feedback.ipynb       # Please give feedback in here\n",
    "│   README.ipynb         # This file\n",
    "│   agent.py             # Contains functions that govern the policy\n",
    "│   reacher.py           # Defines the reacher environment\n",
    "│   train.py             # Contains training and testing functions\n",
    "│   utils.py             # Contains useful functions \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c1cfb",
   "metadata": {},
   "source": [
    "Please consult ```README.md``` for more details the assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb8a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path # to find directory\n",
    "work_dir = Path().cwd()/'results'\n",
    "import os\n",
    "\n",
    "import train as t # for training\n",
    "import utils as u # helper functions\n",
    "\n",
    "import numpy as np # The numpy library can be used for math functions\n",
    "import torch # Used to manage policy and learning\n",
    "from IPython.display import Video, display, HTML # to display videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e36a57",
   "metadata": {},
   "source": [
    "# 2. Cartpole <a id='2.'></a>\n",
    "\n",
    "The Cartpole environment consists of a cart and a pole mounted on top of it, as shown in Figure 1. The cart can move either to the left or to the right. The goal is to balance the pole in a vertical position in order to prevent it from falling down. The cart should also stay within limited distance from the center (trying to move outside screen boundaries is considered a failure).\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/cartpole.png\" width=\"400px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 1: The Cartpole environment  </figcaption>\n",
    "</figure>\n",
    "\n",
    "The state and the observation are four element vectors:\n",
    "\n",
    "$$\n",
    "o=s=\\left(\\begin{array}{c}\n",
    "x \\\\\n",
    "\\dot{x} \\\\\n",
    "\\theta \\\\\n",
    "\\dot{\\theta}\n",
    "\\end{array}\\right) \\text {, }\n",
    "$$\n",
    "\n",
    "where $x$ is the position of the cart, $\\dot{x}$ is its velocity, $\\theta$ is the angle of the pole w.r.t. the vertical axis, and $\\dot{\\theta}$ is the angular velocity of the pole.\n",
    "\n",
    "In the standard formulation, a reward of 1 is given for every timestep the pole remains balanced. Upon failing (the pole falls) or completing the task, an episode is finished.\n",
    "\n",
    "The training script will record videos of the agent’s learning progress during training, and the recorded videos are saved to ```results/video/CartPole-v0/train```. By default, the training information is saved to ```results/logging/CartPole-v0_{seed}.csv```. When the training is finished, the models are saved to ```results/model/Cartpole-v0_params.pt```. The models can be tested by setting ```testing=true``` in ```cfg_args```, and if the models are saved to a different path, you can use ```model_path=<YOUR MODEL PATH>``` to indicate it. Videos of the agent’s behaviour during testing are saved to ```results/video/CartPole-v0/test```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5bb7ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Training a Model for Simple Cartpole Environment (10 points) </h3> \n",
    "\n",
    "This task requires you to train a model for the cartpole environment with 100 timesteps per episode and then report the training plot using 'Weights \\& Biases'. Then test the model for 1000 timesteps and report average reward. To do this, you can simply run the code in the cells below. \n",
    "\n",
    "To see a full list of options that can be passed through ```cfg_args``` consult the configuation file found in ```cfg/```.\n",
    "\n",
    "- **1st** Run training over 100 steps per episode by using ```t.train``` function. See the cell below. \n",
    "- **2nd:** Export the training plot ```episodesep_reward``` from logged data (.csv format).\n",
    "- **3rd:** Run testing over 500 steps by using ```t.test``` function. See the cell below. See the cell below. Notice ```max_episode_steps``` parameter. \n",
    "- **4th:** Report the average reward after testing the model.\n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0481176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.train(cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', \n",
    "        cfg_args=dict(seed=1, max_episode_steps=100)) # < 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8cd415-c26a-44cf-9f76-f80237cdd918",
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_reward(Path().cwd()/'results'/'logging'/'CartPole-v1_1.csv', 'CartPole')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d967ec4",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND REPORT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0230ba0",
   "metadata": {},
   "source": [
    "The command below will evaluate the trained model in 10 episodes and report the average reward (and episode length) for these 10 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95418d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.test(episodes=10, \n",
    "       cfg_path=Path().cwd()/'cfg'/'cartpole_v1.yaml', \n",
    "       cfg_args=dict(testing=True, seed=None, max_episode_steps=1000, use_wandb=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da8596",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND REPORT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a689b7",
   "metadata": {},
   "source": [
    "The agent acting in the environment can be seen using the following command. Change the ```path``` to pick the episode you want to visualize. Bear in mind by default video saving for training is taken every 50 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7bc0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train Result\n",
    "video_dir = work_dir/'video'/'CartPole-v1'/'train'\n",
    "\n",
    "# List all MP4 files in the directory\n",
    "mp4_files = [file for file in os.listdir(video_dir) if file.endswith(\".mp4\")]\n",
    "frame_colors = ['#FF5733', '#33FF57', '#5733FF', '#FFFF33', '#33FFFF', '#FF33FF']\n",
    "# Display each MP4 file\n",
    "for i, mp4_file in enumerate(mp4_files):\n",
    "    video_path = os.path.join(video_dir, mp4_file)\n",
    "    video = Video(video_path, embed=True, html_attributes=\"loop autoplay\", width=200, height=100)\n",
    "    frame_color = frame_colors[i % len(frame_colors)]\n",
    "    video_frame = HTML(f'<div style=\"width: 200px; height: 100px;; border: 1px solid #FF5733;\">{video._repr_html_()}</div>')\n",
    "    print(\"test/\",mp4_file)\n",
    "    display(video_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0e835-79ad-47c7-9984-bfc705927d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Result\n",
    "\n",
    "video_dir = work_dir/'video'/'CartPole-v1'/'test'\n",
    "\n",
    "# List all MP4 files in the directory\n",
    "mp4_files = [file for file in os.listdir(video_dir) if file.endswith(\".mp4\")]\n",
    "frame_colors = ['#FF5733', '#33FF57', '#5733FF', '#FFFF33', '#33FFFF', '#FF33FF']\n",
    "# Display each MP4 file\n",
    "for i, mp4_file in enumerate(mp4_files):\n",
    "    video_path = os.path.join(video_dir, mp4_file)\n",
    "    video = Video(video_path, embed=True, html_attributes=\"loop autoplay\", width=200, height=100)\n",
    "    frame_color = frame_colors[i % len(frame_colors)]\n",
    "    video_frame = HTML(f'<div style=\"width: 200px; height: 100px;; border: 1px solid #5733FF;\">{video._repr_html_()}</div>')\n",
    "    print(\"test/\",mp4_file)\n",
    "    display(video_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9298a34",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.1</b> Learning (10 points) </h3> \n",
    "\n",
    "Test the trained model from Task 1 five times with different random seeds. Did the same model, trained to balance for 100 timesteps, learn to always balance the pole for 1000 timesteps? Why/why not?\n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5678a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d817076",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b5ed9",
   "metadata": {},
   "source": [
    "<a id='T2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 2.</b> Investigating Training Performance (10 points) </h3> \n",
    "\n",
    "Repeat the experiment in Task 1 five times, each time training the model from scratch with 100 timesteps and testing it for 1000 timesteps. Use a different seed number for each training/testing cycle. You can use the box below to write a small script to do this. Use the result textbox below to report the average test reward for each repeat. \n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b9768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f15cc05",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND REPORT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ff4274",
   "metadata": {},
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 2.1</b> Analysis of Training Performance (15 points) </h3> \n",
    "\n",
    "Are the behavior and performance of the trained models the same every time? Why/why not? Analyze the causes briefly.\n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2bcdb",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b248abf",
   "metadata": {},
   "source": [
    "<a id='Q3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 2.2</b> Stochasticity (10 points) </h3> \n",
    "\n",
    "What are the implications of this stochasticity, when it comes to comparing reinforcement learning algorithms to each other? Please explain.\n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b41e22",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842afa68",
   "metadata": {},
   "source": [
    "# Reacher <a id='3.'></a>\n",
    "\n",
    "Now we will focus on designing a reward function for a different environment, the Reacher environment, where a two-joint manipulator needs to reach a goal (see Figure 2).\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/reacher.png\" width=\"200px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 2: The Reacher environment  </figcaption>\n",
    "</figure>\n",
    "\n",
    "The Cartesian ($x$, $y$) position of the end-effector of the manipulator can be determined following the equation:\n",
    "\n",
    "$$\n",
    " x = L_1 \\sin(\\theta_0)+L_2 \\sin(\\theta_0+\\theta_1)\\\\\n",
    " y = -L_1 \\cos(\\theta_0)-L_2 \\cos(\\theta_0+\\theta_1)\n",
    "$$\n",
    "\n",
    "where $L1 = 1$, $L2 = 1$ are the lengths, and $\\theta_0$, $\\theta_1$ the joint angles of the first and second links respectively. The state (and observation) in this environment is the two element vector:\n",
    "\n",
    "$$\n",
    "o=s=\\left(\\begin{array}{c}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\end{array}\\right) \\text {, }\n",
    "$$\n",
    "\n",
    "The action space now consists of 5 \"options\"; 4 correspond rotating the first/second joint left/right, and the final one performs no motion at all (the configuration doesn’t change). The episode terminates when the agent reaches the target position, marked in red. Now, let us design a custom reward function and use it for training the RL agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e138b75d",
   "metadata": {},
   "source": [
    "<a id='T3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 3.</b> Reward Functions (20 points) </h3> \n",
    "\n",
    "Below two classes are shown that modify the reward function of the reacher function provided in ```reacher.py```. Edit the function ```get_reward``` _below_ (not in ```reacher.py```) in both classes. For each class, write a reward function to incentivise the agent to learn the following behaviors:\n",
    "\n",
    "Class 1) ```SpinningReacherEnv```: Keep the manipulator rotating clockwise continuously (w.r.t. angle θ_0). You can use a lower number of training episodes for this, e.g. train(cfg_args=dict(env_name='SpinningReacher-v0', train_episodes=200), overrides=['env=reacher_v1'])\n",
    "\n",
    "Class 2) ```TargetReacherEnv```: Reach the goal point located in x = [1.0,1.0] (marked in red). Use at least 500 training episodes.\n",
    "    \n",
    "Train one model for each behavior. \n",
    "\n",
    "**Hint:** Use the observation vector to get the quantities required to compute the new reward (such as the position of the manipulator). You can get the Cartesian position of the end-effector with ```self.get_cartesian_pos(state)```.\n",
    "            \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa70b97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf76fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from reacher import ReacherEnv\n",
    "from typing import Optional\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "class SpinningReacherEnv(ReacherEnv):\n",
    "    def __init__(self, render_mode: Optional[str] = None, max_episode_steps=200):\n",
    "        super().__init__(render_mode=render_mode, max_episode_steps=max_episode_steps)\n",
    "        \n",
    "    def get_reward(self, prev_state, action, next_state):\n",
    "        # TODO: Task 3: Implement and test the first reward function\n",
    "        ########## Your code starts here ##########\n",
    "        return 1\n",
    "        ########## Your codes end here ########## \n",
    "        \n",
    "    \n",
    "register(\"SpinningReacher-v0\",\n",
    "        entry_point=\"%s:SpinningReacherEnv\"%__name__,\n",
    "        max_episode_steps=200)\n",
    "\n",
    "class TargetReacherEnv(ReacherEnv):\n",
    "    def __init__(self, render_mode: Optional[str] = None, max_episode_steps=200):\n",
    "        super().__init__(render_mode=render_mode, max_episode_steps=max_episode_steps)\n",
    "        \n",
    "    def get_reward(self, prev_state, action, next_state):\n",
    "        # TODO: Task 3: Implement and test the second reward function\n",
    "        ########## Your code starts here ##########\n",
    "        return 1\n",
    "        ########## Your codes end here ########## \n",
    "        \n",
    "register(\"TargetReacher-v0\",\n",
    "        entry_point=\"%s:TargetReacherEnv\"%__name__,\n",
    "        max_episode_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ea45c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.train(cfg_path=Path().cwd()/'cfg'/'reacher_v1.yaml', \n",
    "      cfg_args=dict(env_name='SpinningReacher-v0', train_episodes=200, seed=1)) # < 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4cb332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.test(episodes=10, cfg_path=Path().cwd()/'cfg'/'reacher_v1.yaml', \n",
    "       cfg_args=dict(env_name='SpinningReacher-v0', testing=True,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383507bc",
   "metadata": {},
   "source": [
    "The agent acting in the environment can be seen using the following command. Change the ```path``` to pick the episode you want to visualize. Bear in mind by default video saving for training is taken every 50 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555bc002",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(work_dir/'video'/'SpinningReacher-v0'/'test'/f'ex1-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c258a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.train(cfg_path=Path().cwd()/'cfg'/'reacher_v1.yaml', \n",
    "      cfg_args=dict(env_name='TargetReacher-v0', train_episodes=200, seed=1)) # < 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.test(episodes=10, cfg_path=Path().cwd()/'cfg'/'reacher_v1.yaml', \n",
    "       cfg_args=dict(env_name='TargetReacher-v0', seed=None, testing=True,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67152513",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(work_dir/'video'/'TargetReacher-v0'/'test'/f'ex1-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c384e863",
   "metadata": {},
   "source": [
    "<a id='T4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 4.</b> Visualizing Behavior (10 points) </h3> \n",
    "\n",
    "Now, let us visualize the reward function for the second behavior (reaching the goal [1,1]). Plot the values of the second reward function from Task 3 and the learned best action as a function of the state (the joint positions). Use the code below as a starting point. After plotting, answer the questions below.\n",
    "            \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b3387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "from agent import Agent, Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"TargetReacher-v0\" \n",
    "resolution = 101  # Resolution of the policy/reward image\n",
    "\n",
    "# Load policy from default path to plot\n",
    "policy_dir = Path().cwd()/'results'/'model'/f'{env_name}_params.pt'\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Create a gym environment\n",
    "env = gym.make(env_name)\n",
    "\n",
    "action_space_dim = u.get_space_dim(env.action_space)\n",
    "observation_space_dim = u.get_space_dim(env.observation_space)\n",
    "policy = Policy(observation_space_dim, action_space_dim)\n",
    "\n",
    "if policy_dir:\n",
    "    policy.load_state_dict(torch.load(policy_dir))\n",
    "    print(\"Loading policy from\", policy_dir)\n",
    "else:\n",
    "    print(\"Plotting a random policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid and initialize arrays to store rewards and actions\n",
    "npoints = resolution\n",
    "state_range = np.linspace(-np.pi, np.pi, npoints)\n",
    "rewards = np.zeros((npoints, npoints))\n",
    "actions = np.zeros((npoints, npoints), dtype=np.int32)\n",
    "\n",
    "# Loop through state[0] and state[1]\n",
    "for i,th1 in enumerate(state_range):\n",
    "    for j,th2 in enumerate(state_range):\n",
    "        # Create the state vector from th1, th2\n",
    "        state = np.array([th1, th2])\n",
    "\n",
    "        # Query the policy and find the most probable action\n",
    "        with torch.no_grad():\n",
    "            action_dist, _ = policy(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        action_probs = action_dist.probs.numpy()\n",
    "\n",
    "        # TODO: Task 4: 1. What's the best action, according to the policy?\n",
    "        # .             2. Compute the reward given state\n",
    "        ########## Your code starts here ##########\n",
    "        # Use the action probabilities in the action_probs vector\n",
    "        # (it's a numpy array)\n",
    "        actions[i, j] = 0\n",
    "        \n",
    "        rewards[i,j] = 0 \n",
    "        ########## Your code ends here ##########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the reward plot\n",
    "num_ticks = 10\n",
    "tick_skip = max(1, npoints // num_ticks)\n",
    "tick_shift = 2*np.pi/npoints/2\n",
    "tick_points = np.arange(npoints)[::tick_skip] + tick_shift\n",
    "tick_labels = state_range.round(2)[::tick_skip]\n",
    "\n",
    "sns.heatmap(rewards)\n",
    "plt.xticks(tick_points, tick_labels, rotation=45)\n",
    "plt.yticks(tick_points, tick_labels, rotation=45)\n",
    "plt.xlabel(\"J2\")\n",
    "plt.ylabel(\"J1\")\n",
    "plt.title(\"Reward\")\n",
    "plt.suptitle(\"Rewards in %s\" % env_name)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the policy plot\n",
    "cmap = sns.color_palette(\"deep\", action_space_dim)\n",
    "sns.heatmap(actions, cmap=cmap, vmin=0, vmax=action_space_dim-1)\n",
    "plt.xticks(tick_points, tick_labels, rotation=45)\n",
    "plt.yticks(tick_points, tick_labels, rotation=45)\n",
    "colorbar = plt.gca().collections[0].colorbar\n",
    "ticks = np.array(range(action_space_dim))*((action_space_dim-1)/action_space_dim)+0.5\n",
    "colorbar.set_ticks(ticks)\n",
    "if env.spec.id == \"Reacher-v1\":\n",
    "    # In Reacher, we can replace 0..4 with more readable labels\n",
    "    labels = [\"J1+\", \"J1-\", \"J2+\", \"J2-\", \"Stop\"]\n",
    "else:\n",
    "    labels = list(map(str, range(action_space_dim)))\n",
    "colorbar.set_ticklabels(labels)\n",
    "plt.xlabel(\"J2\")\n",
    "plt.ylabel(\"J1\")\n",
    "plt.title(\"Best action\")\n",
    "plt.suptitle(\"Best action in %s\" % env_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45dc911",
   "metadata": {},
   "source": [
    "<a id='3.2.1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 4.1</b> Achieved Performance (5 points) </h3> \n",
    "\n",
    "Where are the highest and lowest reward achieved?\n",
    "            \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011e57b",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77cbd95",
   "metadata": {},
   "source": [
    "<a id='Q5'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 4.2</b> Analysis of Behaviour (10 points) </h3> \n",
    "\n",
    "Did the policy learn to reach the goal from every possible state (manipulator configuration) in an optimal way (i.e. with lowest possible number of steps)? Why/why not?\n",
    "            \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2b7fd",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50da4d",
   "metadata": {},
   "source": [
    "# 4. Submitting <a id='4.'></a>\n",
    "Ensure all tasks and questions (in ```ex1.ipynb```) are answered and the relevant plots are recorded in the relevant places. Details about attaching images and figures can be found below. The relevant graphs to be included for this assignment are:\n",
    "- Task 1, CartPole ```episodesep_reward``` plot from logged csv file\n",
    "- x2 Task 4 reward plots\n",
    "\n",
    "Ensure the correct model files are saved:\n",
    "- results/model/CartPole-v1_params.pt\n",
    "- results/model/SpinningReacher-v0_params.pt\n",
    "- results/model/TargetReacher-v0_params.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3b7e3-49a1-4ca6-8484-e9fd422271be",
   "metadata": {},
   "source": [
    "## 4.1 Feedback <a id='4.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer to the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29381b17-ce64-4a1c-b16c-e82ea2a0af96",
   "metadata": {},
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc91b59-0048-475d-a679-8e976ba0c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbefa77-e4a4-48ec-8049-e52b40fff463",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35321f8a-a156-4953-ba84-689b6e53a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = None # Student Task 1. Implementing Q-Learning (25 points)\n",
    "T2 = None # Student Task 2. Visualizing the Value Function (10 points)\n",
    "Q2_1 = None # Student Question 2.1 Analyzing the Value Function Heatmap (15 points)\n",
    "T3 = None # Student Task 3. Investigating Initial Values (10 points)\n",
    "Q3_1 = None # Student Question 3.1 Analyzing Initial Values (5 points)\n",
    "Q3_2 = None # Student Question 3.2 Exploration (15 points)\n",
    "T4 = None # Student Task 4. Using Q-Learning on the Lunar Lander Environment (5 points)\n",
    "Q4_1 = None # Student Question 4.1 Lunar Lander Performance (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4983ba-5031-460d-bdb3-e7cd708ed203",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed5d535-89b1-4852-81ef-541a0e1d6dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = None # Student Task 1. Implementing Q-Learning (25 points)\n",
    "T2 = None # Student Task 2. Visualizing the Value Function (10 points)\n",
    "Q2_1 = None # Student Question 2.1 Analyzing the Value Function Heatmap (15 points)\n",
    "T3 = None # Student Task 3. Investigating Initial Values (10 points)\n",
    "Q3_1 = None # Student Question 3.1 Analyzing Initial Values (5 points)\n",
    "Q3_2 = None # Student Question 3.2 Exploration (15 points)\n",
    "T4 = None # Student Task 4. Using Q-Learning on the Lunar Lander Environment (5 points)\n",
    "Q4_1 = None # Student Question 4.1 Lunar Lander Performance (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7670bd2-6bff-418a-bf25-fabf1a3e5396",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "And other feedback you think is worth including. Type in the box below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4330ed5-2f15-4580-bf05-34e9aaf0004c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Please use the following section to record references.\n",
    "# References <a id='5.'></a>\n",
    "\n",
    "[1] Sutton, Richard S., and Andrew G. Barto. \"Reinforcement Learning: An Introduction (in progress).\" London, England (2017). http://incompleteideas.net/book/RLbook2018.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

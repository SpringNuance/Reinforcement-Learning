{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53274fb5",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 4 - Function Approximators Part 1: Radial Basis Functions </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Sep 4, 2023 - Nov 30, 2023</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Task environments </a>\n",
    "* <a href='#1.2'> 1.2 Learning Objectives </a>\n",
    "* <a href='#1.3'> 1.3 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Approximate with non-linear features</a>\n",
    "* <a href='#2.1'> 2.1 Radial Basis Functions</a>\n",
    "* <a href='#2.2'> 2.2 Sklearn RBF function</a>\n",
    "* <a href='#3.'> 3. Submitting </a>\n",
    "* <a href='#3.1'> 3.1 Feedback </a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implementing Q-learning using Function Approximation (20 points) </a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> Considering Linear Features (10 points) </a>\\\n",
    "<a href='#Q2'><b>Student Question 1.2</b> Experience Replay Buffer (10 points) </a>\\\n",
    "<a href='#Q3'><b>Student Question 1.3</b> Grid-based vs Function Approximators (10 points) </a>\\\n",
    "<a href='#T2'><b>Student Task 2.</b> Visualizing the Policy (10 points)</a>\n",
    "    \n",
    "**Total Points:** 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30395ea",
   "metadata": {},
   "source": [
    "# 1. Introduction <a id='1.'></a>\n",
    "\n",
    "In many real world scenarios, the dimensionality of the state space may be too high to compute and store the Q-values for each possible state and action in a Q-table. Instead, the state value and action value functions can be learned by using a function approximator, such as a radial basis functions (RBFs). In this exercise, you will start with handcrafted features and then move to radial basis functions in the **Cartpole** environment.\n",
    "\n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"imgs/cartpole.png\" width=\"300px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 1: The Cartpole environment. </figcaption>\n",
    "</figure>\n",
    "\n",
    "Useful Sources:\n",
    "\n",
    "- Sutton, Richard S., and Andrew G. Barto. \"Reinforcement Learning: An Introduction (in progress).\" London, England (2017). http://incompleteideas.net/book/RLbook2018.pdf\n",
    "\n",
    "## 1.1 Task environments <a id='1.1'></a>\n",
    "\n",
    "In this part, we will specifically use RBF-based Q-learning to solve cartpole task. In this environment, a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart. \n",
    "\n",
    "You can find the detailed description of the task through this link: https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "\n",
    "\n",
    "## 1.2 Learning Objectives: <a id='1.2'></a>\n",
    "- Understand why and how function approximators can be used for Q-learning\n",
    "- Develop intuition behind function approximators: radial basis functions\n",
    "\n",
    "## 1.3 Code Structure & Files <a id='1.3'></a>\n",
    "\n",
    "```ex4_rbf.ipynb``` is the file needed to be modified for this part of the assignment. \n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER TASK IN ```ex4_dqn.ipynb```** </span>\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ‚îÄcfg                            # Config files for environments\n",
    "‚îú‚îÄ‚îÄ‚îÄimgs                           # Images used in notebook\n",
    "‚îú‚îÄ‚îÄ‚îÄresults\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄCartPole-v1\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄlogging                \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄlogging.pkl        # Contains logged data\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄmodel              \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ*dqn.pt            # Contains trained model for dqn task\n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ*rbf.pkl           # Contains trained model for rbf task\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo                   # Videos saved\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ*cartpole_rbf.png       # Contains training performance plot for rbd\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ*cartpole_hd.png        # Contains training performance plot for handcrafted feature\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄLunarLander-v2\n",
    "‚îÇ   ex4_dqn.ipynb                   # 2nd assignment file containing tasks <---------\n",
    "‚îÇ   ex4_rbf.ipynb                   # 1st assignment file containing tasks <---------This task\n",
    "‚îÇ   train.py                        # Contains train and test functions \n",
    "‚îÇ   utils.py                        # Contains useful functions \n",
    "‚îî‚îÄ‚îÄ‚îÄbuffer.py                       # Contains buffer functions\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c5f5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Approximate with non-linear features <a id='2.'></a>\n",
    "## 2.1 Radial Basis Functions <a id='2.1'></a>\n",
    "A radial basis function (RBF) is a real-valued function $\\phi$ that maps a vector $\\boldsymbol{x} \\in \\mathbb{R}^d$ into a real-value $\\phi : \\mathbb{R}^d \\mapsto \\mathbb{R}$. The RBF $\\phi(\\boldsymbol{x} )$ = $\\phi(||\\boldsymbol{x} ||)$ acts on a distance (radial) and a basis function $\\phi$ (e.g. Gaussian function). The RBFs are commonly used as kernels in machine learning. The RBF kernel is defined as a $K(\\boldsymbol{x} , \\boldsymbol{x} ‚Ä≤) = „Äà\\phi(\\boldsymbol{x}),\\phi(\\boldsymbol{x}‚Ä≤)„Äâ$, where $\\phi$ is the Gaussian function. Therefore, the RBF kernel is defined as\n",
    "\n",
    "$$\n",
    "K\\left(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime}\\right)=\\exp \\left(-\\frac{-\\left\\|\\boldsymbol{x}-\\boldsymbol{x}^{\\prime}\\right\\|^2}{2 \\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "where the parameter $\\phi^2$ is the variance and gives shape to the Gaussian function. \n",
    "\n",
    "On the exercise you are given a ```featurizer``` which uses an ```RBFSampler```. The ```RBFSampler``` uses a Monte Carlo approximation of the RBF kernel and generates samples with the specified variance. The ```RBFSampler``` has been trained on a predefined dataset. Thus, the RBF kernel will map the state $x \\in \\mathbb{R}$ d to the distance to the trained dataset. Therefore, an input state will be transformed to as many features as samples $N$ are specified in the ```RBFSampler``` $\\phi : \\mathbb{R}^d \\mapsto \\mathbb{R}^N $. \n",
    "\n",
    "In a nutshell, the ```featurizer``` function maps low-dimensional states to a high-dimensional representation, which reveals properties that haven‚Äôt been shown in the original low-dimensional space, e.g., for the Cartpole environment, it maps the original four-dimensional states to a 230-dimensional vector.\n",
    "\n",
    "## 2.2 Sklearn RBF function<a id='2.2'></a>\n",
    "In this excercise, we use scikit-learn library to implement the RBF approximation by doing following steps:\n",
    "\n",
    "- RBFSampler: A RBF kernel will map feature into higher dimensions using random Fourier features.\n",
    "- FeatureUnion: Concatenates results of multiple transformer objects.\n",
    "- SGDRegressor: Linear model fitted by minimizing a regularized empirical loss with SGD.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5511d36c",
   "metadata": {},
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implementing Q-learning using Function Approximation (20 points) </h3> \n",
    "\n",
    "Implement Q-learning using function approximation. Also implement $\\epsilon$-greedy action selection. Test two different features for state representations:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a) handcrafted feature vector $\\phi(s) = [s, |s|]^T$,(use the ```featurizer``` inside the ```RBFAgent``` class).\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) radial basis function representations (use the ```featurizer``` inside the ```RBFAgent``` class).\n",
    "\n",
    "For this task you need to modify functions ```featurize```, ```get_action```, and ```update``` inside the ```RBFAgent``` class. Test the implementation on the Cartpole environment by running the ```train(agent)``` script with default configs. You can test the trained model with the ```test(agent, cfg_args=dict(save_video=true))``` script. \n",
    "    \n",
    "**See Figure 2 for an example training performance plot for Task1(b). Save the training performance plots for both (a) and (b), and check if they are in the right place (the paths please refer to <a href='#3.'>Submitting<a>).**\n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"imgs/rbf.png\" width=\"300px\">\n",
    "<figcaption style=\"text-align: center\">Figure 2: The training performance plot for Task 1 (b) might look something like this.</figcaption>\n",
    "</figure>\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62817342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from buffer import Batch\n",
    "import utils as u\n",
    "import train as t\n",
    "\n",
    "from IPython.display import Video # to display videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee3fb4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RBFAgent(object):\n",
    "    def __init__(self, n_actions, gamma=0.98, batch_size=32):\n",
    "        self.scaler = None\n",
    "        self.featurizer = None\n",
    "        self.q_functions = None\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "        self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        # Draw some samples from the observation range and initialize the scaler (used to normalize data)\n",
    "        obs_limit = np.array([4.8, 5, 0.5, 5])\n",
    "        samples = np.random.uniform(-obs_limit, obs_limit, (1000, obs_limit.shape[0]))\n",
    "        \n",
    "        # calculate the mean and var of samples, used later to normalize training data\n",
    "        self.scaler = StandardScaler().fit(samples) \n",
    "\n",
    "        # Initialize the RBF featurizer\n",
    "        self.featurizer = FeatureUnion([\n",
    "            (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "            (\"rbf2\", RBFSampler(gamma=2.0, n_components=80)),\n",
    "            (\"rbf3\", RBFSampler(gamma=1.0, n_components=50)),\n",
    "        ])\n",
    "        self.featurizer.fit(self.scaler.transform(samples))\n",
    "\n",
    "        # Create a value approximator for each action dimension\n",
    "        self.q_functions = [SGDRegressor(learning_rate=\"constant\", max_iter=500, tol=1e-3)\n",
    "                       for _ in range(self.n_actions)]\n",
    "\n",
    "        # Initialize it to whatever values; implementation detail\n",
    "        for q_a in self.q_functions:\n",
    "            q_a.partial_fit(self.featurize(samples), np.zeros((samples.shape[0],)))\n",
    "\n",
    "    def featurize(self, state):\n",
    "        \"\"\" Map state to a higher dimension for better representation.\"\"\"\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.reshape(1, -1)\n",
    "        \n",
    "        # TODO: Task 1, choose which feature to use. \n",
    "        # Task 1a: implement the manual features here \n",
    "        ########## You code starts here #########\n",
    "        #manual_feature =\n",
    "        ########## You code ends here #########\n",
    "\n",
    "        \n",
    "        # Task 1b: implement the RBF features transformation here \n",
    "        # it will map a state to a higher dimension (100+80+50)\n",
    "        ########## You code starts here #########\n",
    "        #rbf_feature = \n",
    "        ########## You code ends here #########\n",
    "\n",
    "        #Task 1:return manual_feature or return rbf_feature\n",
    "        ########## You code starts here #########\n",
    "        #feature = manual_feature #(task 1 a)\n",
    "        #feature = rbf_feature #(task 1 b)\n",
    "        ########## You code ends here #########\n",
    "        \n",
    "        return feature\n",
    "    \n",
    "    def get_action(self, state, epsilon=0.0):\n",
    "        # TODO: Task 1: Implement epsilon-greedy policy\n",
    "        # Hints:\n",
    "        # 1. self.q_functions is a list which defines a q function for each action dimension\n",
    "        # 2. for each q function, use predict(feature) to obtain the q value\n",
    "        ########## Your code starts here ##########\n",
    "\n",
    "        \n",
    "        ########## Your code ends here #########\n",
    "        return action\n",
    "\n",
    "\n",
    "    def _to_squeezed_np(self, batch:Batch) -> Batch:\n",
    "        \"\"\" A simple helper function squeeze data dimension and cover data format from tensor to np.ndarray.\"\"\"\n",
    "        _ret = {}\n",
    "        for name, value in batch._asdict().items():\n",
    "            if isinstance(value, dict): # this is for extra, which is a dict\n",
    "                for k, v in value.items():\n",
    "                    value[k] = v.squeeze().numpy()\n",
    "                _ret[name] = value\n",
    "            else:\n",
    "                _ret[name] = value.squeeze().numpy()\n",
    "        return Batch(**_ret)\n",
    "        \n",
    "    def update(self, buffer):\n",
    "        # batch is a namedtuple, which has state, action, next_state, not_done, reward\n",
    "        # you can access the value be batch.<name>, e.g, batch.state\n",
    "        batch = buffer.sample(self.batch_size) \n",
    "        # Hint:\n",
    "        #    state = batch.state\n",
    "        #    action = batch.action \n",
    "        #    next_state = batch.next_state\n",
    "        #    reward = batch.reward \n",
    "        #    not_done = batch.not_done \n",
    "        \n",
    "        # the returned batch is a namedtuple, where the data is torch.Tensor\n",
    "        # we first squeeze dim and then covert it to Numpy array.\n",
    "        batch = self._to_squeezed_np(batch)\n",
    "\n",
    "        # TODO: Task 1, update q_functions\n",
    "        # Hints: \n",
    "        # 1. featurize the state and next_state\n",
    "        # 2. calculate q_target (check q-learning)\n",
    "        # 3. self.q_functions is a list which defines a q function for each action dimension\n",
    "        #    for each q function, use q.predict(featurized_state) to obtain the q value \n",
    "        # 4. remember to use not_done to mask out the q values at terminal states (treat them as 0)\n",
    "        ########## You code starts here #########\n",
    "        \n",
    "        ########## You code ends here #########\n",
    "        \n",
    "        # Get new weights for each action separately\n",
    "        for a in range(self.n_actions):\n",
    "            # Find states where `a` was taken\n",
    "            idx = batch.action == a\n",
    "\n",
    "            # If a not present in the batch, skip and move to the next action\n",
    "            if np.any(idx):\n",
    "                act_states = f_state[idx]\n",
    "                act_targets = q_tar[idx]\n",
    "\n",
    "                # Perform a single SGD step on the Q-function params to update the q_function corresponding to action a\n",
    "                self.q_functions[a].partial_fit(act_states, act_targets)\n",
    "\n",
    "        return {}\n",
    "\n",
    "    def save(self, fp):\n",
    "        path = fp/'rbf.pkl'\n",
    "        u.save_object(\n",
    "            {'q': self.q_functions, 'featurizer': self.featurizer},\n",
    "            path\n",
    "        )\n",
    "\n",
    "    def load(self, fp):\n",
    "        path = fp/'rbf.pkl'\n",
    "        d = u.load_object(path)\n",
    "\n",
    "        self.q_functions = d['q']\n",
    "        self.featurizer = d['featurizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb219433-160a-4c5b-b1bf-6c654a57e703",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h4><b>Student Task 1.(a)</b> Linear model with handcraft feature </h4> \n",
    "\n",
    "First, let's test Q-learning with handcrafted features. We define the feature as :\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\phi(s) = [s, |s|]^T$\n",
    "\n",
    "Then we will feed this feature into a linear model to predict corresponding Q-values.\n",
    "\n",
    "For this Task, implement the code in ```class RBFAgent```:\n",
    "    \n",
    "1. First, you need to modify functions ```featurize```, to convert the state $s$ from encironment to $[s, |s|]^T$.\n",
    "2. Modify the function ```get_action```, use ```self.q_functions``` to select the action has the highest return.\n",
    "3. Modify the function ```update```, based on the Q-learning objective to update the Q-function (linear models).\n",
    "\n",
    "After the modifying, test the implementation on the Cartpole environment by running the ```train(agent)``` script with default configs. You can test the trained model with the ```test(agent, cfg_args=dict(save_video=true))``` script. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd6467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After the modifying, run\n",
    "# init agent\n",
    "with open(Path().cwd()/'cfg'/'cartpole_rbf.yaml', 'r') as f:\n",
    "    cfg = t.Struct(**yaml.safe_load(f))\n",
    "        \n",
    "agent = RBFAgent(n_actions=cfg.n_actions, gamma=cfg.gamma, batch_size=cfg.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c6566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.train(agent, cfg_path=Path().cwd()/'cfg'/'cartpole_rbf.yaml', cfg_args={}) # < 10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57a812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.test(agent, cfg_path=Path().cwd()/'cfg'/'cartpole_rbf.yaml', cfg_args=dict(save_video=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9ba558-6057-4142-804b-d6bc43d0d709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.plot(save_name='cartpole_hd.png', cfg_path=Path().cwd()/'cfg'/'cartpole_rbf.yaml', cfg_args={}) # < 15 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb6b18",
   "metadata": {},
   "source": [
    "The agent acting in the environment can be seen using the following command. Change the path to pick the episode you want to visualize. Bear in mind by default video saving for training is taken every 500 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b48c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(Path().cwd()/'results'/'CartPole-v1'/'video'/'test'/'ex4_rbf-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f48c8-9ab6-43fb-bce0-19719ce2c9e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h4><b>Student Task 1.(b)</b> RBF approximation </h4> \n",
    "\n",
    "Now, let's test Q-learning with RBF approximation. In this task, we will do a feature transform using the Radial Basis Function. An input state will be transformed to as many features as samples $N$ are specified in the ```RBFSampler``` $\\phi : \\mathbb{R}^d \\mapsto \\mathbb{R}^N $. Then we will feed this $\\mathbb{R}^N $ feature into a linear model to predict corresponding Q-values.\n",
    "\n",
    "For this Task:\n",
    "    \n",
    "1. You need to modify functions ```featurize```, to **convert the state $s$ to a higher dimensional feature using** ```RBFSampler```.\n",
    "\n",
    "2. Run the cell of ```class RBFAgent``` to update the class function\n",
    "\n",
    "3. <span style=\"color:red\"> **IMPORTANT: Directly come to this cell after the modifications, DO NOT RUN the handcraft feature cells again!!!** </span>\n",
    "\n",
    "4. Run the cells below to check the performances of Q-learning with RBF approximation.\n",
    "    \n",
    "After the modifying, test the implementation on the Cartpole environment by running the ```train(agent)``` script with default configs. You can test the trained model with the ```test(agent, cfg_args=dict(save_video=true))``` script. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43c155-a9a9-4d7c-8829-0b472d25314e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After modifying the function 'featurize', \n",
    "# 1. run the cell of class RBFAgent(object) to update the class\n",
    "# 2. After update the class, directly run this cell (Skip the cells above of training hadncraft feature) \n",
    "# 2. now let's rerun the agent\n",
    "\n",
    "with open(Path().cwd()/'cfg'/'cartpole_rbf.yaml', 'r') as f:\n",
    "    cfg = t.Struct(**yaml.safe_load(f))        \n",
    "agent = RBFAgent(n_actions=cfg.n_actions, gamma=cfg.gamma, batch_size=cfg.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f3d51-0bc1-469e-a889-86f217668c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.train(agent, cfg_path=Path().cwd()/'cfg'/'cartpole_rbf.yaml', cfg_args={}) # < 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88396eef-a80e-41dd-bdc7-c5cb67a2e75f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.test(agent, cfg_path=Path().cwd()/'cfg'/'cartpole_rbf.yaml', cfg_args=dict(save_video=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fd3b7-ee1d-46c7-830f-54b295ececa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.plot(save_name='cartpole_rbf.png', cfg_path=Path().cwd()/'cfg'/'cartpole_rbf.yaml', cfg_args={}) # < 5 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445eb823-f602-442f-b4b3-ed6806c18ffe",
   "metadata": {
    "tags": []
   },
   "source": [
    "The agent acting in the environment can be seen using the following command. Change the path to pick the episode you want to visualize. Bear in mind by default video saving for training is taken every 500 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd524780-6930-4de3-ae4b-9157e89760d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(Path().cwd()/'results'/'CartPole-v1'/'video'/'test'/'ex4_rbf-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f725b2e",
   "metadata": {},
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.1</b> Considering Linear Features (10 points) </h3> \n",
    "\n",
    "Would it be possible to learn accurately Q-values for the Cartpole problem using linear features (by passing the state directly to a linear regressor)? Why/why not?\n",
    "    \n",
    "**Hint: directly return 'state' in function 'featurize'**\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc979f",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a8edc7",
   "metadata": {},
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.2</b> Experience Replay Buffer (10 points) </h3> \n",
    "\n",
    "In Task 1, we collect observed states, actions, and rewards into an experience replay buffer. During training we randomly sample mini-batches from this buffer. \n",
    "    \n",
    "1.Why do we use the experience replay buffer, how does it affect the learning performance? \n",
    "    \n",
    "2.Why do we sample the mini-batches randomly?\n",
    "\n",
    "**Hint:** In machine learning, when training a model, we usually assume dataset includes i.i.d(independent and identically distributed) samples from a unknown distribution.\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494cdd70",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4793b0",
   "metadata": {},
   "source": [
    "<a id='Q3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.3</b> Grid-based vs Function Approximators (10 points) </h3> \n",
    "\n",
    "In Exercise 3, we used grid-based Q-learning to balance the Cartpole. Are grid-based methods sample-efficient compared to the RBF function approximation methods? Why/why not?\n",
    "\n",
    "**Hint:** An algorithm is said to be sample-efficient when it requires less samples (data) to reach\n",
    "an optimal performance\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed64fa0e",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b9e0c",
   "metadata": {},
   "source": [
    "<a id='T2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 2.</b> Visualizing the Policy (10 points) </h3> \n",
    "\n",
    "Create a 2D plot of policy (best action in terms of state) learned with RBF in terms of $x$ and Œ∏ for $\\dot{x} = 0$ and $\\dot{\\theta} = 0$, such that Œ∏ is on the horizontal axis and $x$ is on the vertical axis. The plot will be visualized below.\n",
    "\n",
    "**Hint 1 :** You can fix $\\dot{x} = 0$ and $\\dot{\\theta} = 0$ and discretize the state-space for $x$ and $\\theta$.\n",
    "\n",
    "**Hint 2 :** You can directly use the code in ***excercise 1 Task 4*** to visualize the policy.\n",
    "    \n",
    "**Hint 3 :** For CartPole environment state specifications, find them here https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797754bd-2a91-4b68-ab12-84dab27f100a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# define and reload the RBF agent, which is trained with RBF feature!!!!\n",
    "with open(Path().cwd()/'cfg'/'cartpole_rbf.yaml', 'r') as f:\n",
    "    cfg = t.Struct(**yaml.safe_load(f))        \n",
    "agent = RBFAgent(n_actions=cfg.n_actions, gamma=cfg.gamma, batch_size=cfg.batch_size)\n",
    "agent.load(Path().cwd()/'results/CartPole-v1/model/')\n",
    "\n",
    "########## Your code starts here ##########\n",
    "\n",
    "\n",
    "\n",
    "########## Your code ends here #############\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88aa5e4-757d-49c8-9bec-3f23e7bc2f9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Submitting <a id='3.'></a>\n",
    "Ensure all tasks and questions (in ```ex4_dpn.ipynb``` and ```ex4_rbf.ipynb```) are answered and the necessary plots are saved in the appropriate locations. The relevant plots and files need to be included for this part of assignment are:\n",
    "\n",
    "\n",
    "- Training performance plots:\n",
    "  - `cartpole_hd.png`: Training performance of handcrafted feature plots in terms of episode and episodic reward\n",
    "  - `cartpole_rbf.png`: training performance of rbf feature plots in terms of episode and episodic reward \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "  \n",
    "\n",
    "- Model files:\n",
    "  - `rbf.kpl`: Trained model\n",
    "\n",
    "\n",
    "Ensure the correct model files and plots are saved in paths:\n",
    "- ```results/CartPole-v1/cartpole_rbf.png``` Training result for Cartpole environment\n",
    "- ```results/CartPole-v1/cartpole_hd.png```  Training result for Cartpole environment\n",
    "- ```results/CartPole-v1/model/rbf.kpl``` Model for Cartpole environment\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER TASK IN ```ex4_dqn.ipynb```** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbc83ca-dbb0-4024-868e-6718a83e9bb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1 Feedback <a id='3.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer to the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee27da8-ac20-488b-9d05-c03ed1f0f837",
   "metadata": {},
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a4e19d-39b6-466a-8fea-b63f91916cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc128a60-a227-41b5-b6e0-531de2c3df1b",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797f06b-bf2d-4305-abe2-e2773c6fb3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1_a = None   # Implementing Hand feature \n",
    "T2_b = None   # Implementing RBF feature \n",
    "Q1_1 = None # Question 1.1 Considering linear features (10 points)\n",
    "Q1_2 = None # Question 1.2 Experience replay (10 points)\n",
    "Q1_3 = None # Question 1.3 Grid vs Function (10 points)\n",
    "Q2 = None # Question 2 Policy visualization (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6612842-e130-4658-adef-181218023449",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ea255-ca3f-4ee2-860c-147ff88373f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1_a = None   # Implementing Hand feature \n",
    "T2_b = None   # Implementing RBF feature \n",
    "Q1_1 = None # Question 1.1 Considering linear features (10 points)\n",
    "Q1_2 = None # Question 1.2 Experience replay (10 points)\n",
    "Q1_3 = None # Question 1.3 Grid vs Function (10 points)\n",
    "Q2 = None # Question 2 Policy visualization (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77831fe1-f18d-4036-97a9-ad6d1adadfa2",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "Please share any additional feedback, suggestions, or comments you have about the lecture, assignment, or course content. Your input is valuable in helping us improve the learning experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8038fb-e038-4eac-87f3-ff28e0f94e9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

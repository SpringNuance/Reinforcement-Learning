{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53274fb5",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 4 - Function Approximators Part 2: Deep Q Network </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Sep 4, 2023 - Nov 30, 2023</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Task environments </a>\n",
    "* <a href='#1.2'> 1.2 Learning Objectives </a>\n",
    "* <a href='#1.3'> 1.3 Code Structure & Files </a>\n",
    "* <a href='#1.4'> 1.4 Execution time </a>\n",
    "* <a href='#2.'> 2. A (not-so-)deep Q-network</a>\n",
    "* <a href='#3.'> 3. Submitting </a>\n",
    "* <a href='#3.1'> 3.1 Feedback </a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implementing DQN (10 points) </a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> Considering Continuous Action Spaces (5 points) </a>\\\n",
    "<a href='#Q2'><b>Student Question 1.2</b> Continuous Action Spaces Part 1 (15 points) </a>\\\n",
    "<a href='#Q3'><b>Student Question 1.3</b> The Target Network (10 points) </a>\n",
    " \n",
    "\n",
    "**Total Points:** 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3bd96",
   "metadata": {},
   "source": [
    "# 1. Introduction <a id='1.'></a>\n",
    "\n",
    "In various real-world scenarios, dealing with high-dimensional state spaces makes it impractical to compute and store Q-values for every possible state-action pair in a Q-table. To address this challenge, we turn to function approximators. In this assignment, you will progress creating a basic Deep Q-Network (DQN) in the **Cartpole** and **LunarLander** environment.\n",
    "\n",
    "**Please start working on this assignment early since the DQN will take some time to train.**\n",
    "\n",
    "\n",
    "<div style=\"display:flex\">\n",
    "     <div style=\"flex:1;padding-left:100px;\">\n",
    "          <img src=\"imgs/cartpole.png\" width=\"300\"/>\n",
    "         <figcaption style=\"flex:1;padding-left:20px;\">  Figure 1: The Cartpole environment. </figcaption>\n",
    "     </div>\n",
    "     <div style=\"flex:1;padding-left:70px;\">\n",
    "          <img src=\"imgs/lunar_lander.png\" width=\"300\"/>\n",
    "         <figcaption style=\"flex:1;padding-left:20px;\">  Figure 2: The Lunarlander environment. </figcaption>\n",
    "     </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "Useful Sources:\n",
    "\n",
    "- Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint arXiv:1312.5602 (2013). https://arxiv.org/pdf/1312.5602.pdf\n",
    "\n",
    "\n",
    "## 1.1 Task environments: <a id='1.1'></a>\n",
    "In this excercise, we will mainly use DQN for two tasks:\n",
    "- Cartpole(https://gymnasium.farama.org/environments/classic_control/cart_pole/): This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in ‚ÄúNeuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem‚Äù. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "\n",
    "- Lunar Lander (https://gymnasium.farama.org/environments/box2d/lunar_lander/): This environment is a classic rocket trajectory optimization problem. According to Pontryagin‚Äôs maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
    "\n",
    "## 1.2 Learning Objectives: <a id='1.1'></a>\n",
    "- Understand why and how function approximators can be used for Q-learning\n",
    "- Understand the Deep Q-Network RL algorithm\n",
    "\n",
    "## 1.3 Code Structure & Files <a id='1.2'></a>\n",
    "\n",
    "```ex4_dqn.ipynb``` is the file needed to be modified for this part of the assignment.  \n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER TASK IN ```ex4_rbf.ipynb```** </span>\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ‚îÄcfg                            # Config files for environments\n",
    "‚îú‚îÄ‚îÄ‚îÄimgs                           # Images used in notebook\n",
    "‚îú‚îÄ‚îÄ‚îÄresults\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄCartPole-v1\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄlogging                \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄlogging.pkl        # Contains logged data\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄmodel              \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ*dqn.pt            # Contains trained model\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo                   # Videos saved\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ*cartpole_dqn.png       # Contains training performance plot\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄLunarLander-v2\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄlogging                \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄlogging.pkl        # Contains logged data\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄmodel              \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ*dqn.pt            # Contains trained model\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo                   # Videos saved\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ*lunarlander_dqn.png    # Contains training performance plot\n",
    "‚îÇ   ex4_dqn.ipynb                   # 2nd assignment file containing tasks <---------This task\n",
    "‚îÇ   ex4_rbf.ipynb                   # 1st assignment file containing tasks <---------\n",
    "‚îÇ   train.py                        # Contains train and test functions \n",
    "‚îÇ   utils.py                        # Contains useful functions \n",
    "‚îî‚îÄ‚îÄ‚îÄbuffer.py                       # Contains buffer functions\n",
    "```\n",
    "\n",
    "## 1.4 Execution time <a id='1.4'></a>\n",
    "\n",
    "The training of these methods might take more than 40 mins depends on the server. If you have problem of experiment running takes too much time, you can download the jupyter notebook and test it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba7612-914b-4860-b019-7dc32b075b6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. A (not-so-)deep Q-network <a id='2.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b2f2ea",
   "metadata": {},
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implementing DQN (10 points) </h3> \n",
    "\n",
    "Finish the incomplete code in DQNAgent (functions ```update``` and ```get_action```, marked with ```TODO```) to implement a DQN agent.\n",
    "\n",
    "**See Figure 3 for an example training performance plot for cartpole. Save the training performance plots, and check if they are in the right place (the paths please refer to <a href='#3.'>Submitting<a>).**\n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"imgs/dqn.png\" width=\"300px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 3: The training performance plot for cartpole-dqn might look something like this. </figcaption>\n",
    "</figure>\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d14d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch, random, copy, yaml, time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "import utils as u\n",
    "import train as t\n",
    "\n",
    "from IPython.display import Video # to display videos\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c93182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mlp(in_dim, mlp_dims: List[int], out_dim, act_fn=nn.ReLU, out_act=nn.Identity):\n",
    "    \"\"\"Returns an MLP.\"\"\"\n",
    "    if isinstance(mlp_dims, int): raise ValueError(\"mlp dimensions should be list, but got int.\")\n",
    "\n",
    "    layers = [nn.Linear(in_dim, mlp_dims[0]), act_fn()]\n",
    "    for i in range(len(mlp_dims)-1):\n",
    "        layers += [nn.Linear(mlp_dims[i], mlp_dims[i+1]), act_fn()]\n",
    "    # the output layer\n",
    "    layers += [nn.Linear(mlp_dims[-1], out_dim), out_act()]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01a546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_shape, n_actions,\n",
    "                 batch_size=32, hidden_dims=[12], gamma=0.98, lr=1e-3, grad_clip_norm=1000, tau=0.001):\n",
    "        self.n_actions = n_actions\n",
    "        self.state_dim = state_shape[0]\n",
    "        self.policy_net = mlp(self.state_dim, hidden_dims, n_actions).to(device)\n",
    "        self.target_net = copy.deepcopy(self.policy_net)\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=float(lr))\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.grad_clip_norm = grad_clip_norm\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.counter = 0\n",
    "\n",
    "    def update(self, buffer):\n",
    "        \"\"\" One gradient step, update the policy net.\"\"\"\n",
    "        start = time.perf_counter()\n",
    "        self.counter += 1\n",
    "        # Do one step gradient update\n",
    "        batch = buffer.sample(self.batch_size, device=device)\n",
    "        # Hint:\n",
    "        #    state = batch.state\n",
    "        #    action = batch.action \n",
    "        #    next_state = batch.next_state\n",
    "        #    reward = batch.reward \n",
    "        #    not_done = batch.not_done \n",
    "        \n",
    "        # TODO: Task 3: Finish the DQN implementation.\n",
    "        # Hints: 1. You can use torch.gather() to gather values along an axis specified by dim. \n",
    "        #        2. torch.max returns a namedtuple (values, indices) where values is the maximum \n",
    "        #           value of each row of the input tensor in the given dimension dim.\n",
    "        #           And indices is the index location of each maximum value found (argmax).\n",
    "        #        3.  batch is a namedtuple, which has state, action, next_state, not_done, reward\n",
    "        #           you can access the value be batch.<name>, e.g, batch.state\n",
    "        #        4. check torch.nn.utils.clip_grad_norm_() to know how to clip grad norm\n",
    "        #        5. You can go throught the PyTorch Tutorial given on MyCourses if you are not familiar with it. \n",
    "        # calculate the q(s,a)\n",
    "        ########## You code starts here #########\n",
    "     \n",
    "        \n",
    "        ########## You code ends here #########\n",
    "\n",
    "        # update the target network\n",
    "        u.soft_update_params(self.policy_net, self.target_net, self.tau)\n",
    "        \n",
    "        end = time.perf_counter()\n",
    "        update_time = end - start\n",
    "        return {'loss': loss.item(), \n",
    "                'q_mean': qs.mean().item(),\n",
    "                'num_update': self.counter,\n",
    "                'update_time': update_time}\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        # TODO:  Task 3: implement epsilon-greedy action selection\n",
    "        ########## You code starts here #########\n",
    "       \n",
    "        ########## You code ends here #########\n",
    "        return action\n",
    "\n",
    "\n",
    "    def save(self, fp):\n",
    "        path = fp/'dqn.pt'\n",
    "        torch.save({\n",
    "            'policy': self.policy_net.state_dict(),\n",
    "            'policy_target': self.target_net.state_dict()\n",
    "        }, path)\n",
    "\n",
    "    def load(self, fp):\n",
    "        path = fp/'dqn.pt'\n",
    "        d = torch.load(path)\n",
    "        self.policy_net.load_state_dict(d['policy'])\n",
    "        self.target_net.load_state_dict(d['policy_target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d97e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init agent\n",
    "with open(Path().cwd()/'cfg'/'cartpole_dqn.yaml', 'r') as f:\n",
    "    cfg = t.Struct(**yaml.safe_load(f))\n",
    "    \n",
    "agent = DQNAgent(state_shape=cfg.state_shape, n_actions=cfg.n_actions, batch_size=cfg.batch_size, hidden_dims=cfg.hidden_dims,\n",
    "                 gamma=cfg.gamma, lr=cfg.lr, tau=cfg.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc3169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trainig takes approximately 40 mins,depends on server\n",
    "t.train(agent,  cfg_path=Path().cwd()/'cfg'/'cartpole_dqn.yaml',) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9a1164-6330-4e45-ba52-1a84d8a91001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the DQN training plots for the cartpole task\n",
    "t.plot(save_name='cartpole_dqn.png', cfg_path=Path().cwd()/'cfg'/'cartpole_dqn.yaml',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09736f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.test(agent, cfg_path=Path().cwd()/'cfg'/'cartpole_dqn.yaml', cfg_args=dict(save_video=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c63b5ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(Path().cwd()/'results'/'CartPole-v1'/'video'/'test'/'ex4_dqn-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004fa76f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init agent\n",
    "with open(Path().cwd()/'cfg'/'lunarlander_dqn.yaml', 'r') as f:\n",
    "    cfg = t.Struct(**yaml.safe_load(f))\n",
    "    \n",
    "agent = DQNAgent(state_shape=cfg.state_shape, n_actions=cfg.n_actions, batch_size=cfg.batch_size, hidden_dims=cfg.hidden_dims,\n",
    "                 gamma=cfg.gamma, lr=cfg.lr, tau=cfg.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3775e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trainig takes approximately 60 mins,depends on server\n",
    "t.train(agent, cfg_path=Path().cwd()/'cfg'/'lunarlander_dqn.yaml', cfg_args=dict()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a7511-d1aa-4691-be76-406dbccdc739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the DQN training plots for the lunarlander task\n",
    "t.plot(save_name='lunarlander_dqn.png', cfg_path=Path().cwd()/'cfg'/'lunarlander_dqn.yaml', cfg_args=dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2808d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.test(agent, cfg_path=Path().cwd()/'cfg'/'lunarlander_dqn.yaml', cfg_args=dict(save_video=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34889d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(Path().cwd()/'results'/'LunarLander-v2'/'video'/'test'/'ex4_dqn-episode-8.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55896461",
   "metadata": {},
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.1</b> Considering Continuous Action Spaces (5 points) </h3> \n",
    "\n",
    "Can Q-learning be used directly in environments with continuous action spaces?\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b8937",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1395f3d1",
   "metadata": {},
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.2</b> Continuous Action Spaces Part 1 (15 points) </h3> \n",
    "\n",
    "1.Which steps of the algorithm would be difficult to compute in case of a continuous action space?\n",
    "    \n",
    "2.If any, what could be done to solve them?\n",
    "    \n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7481edc",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e680f",
   "metadata": {},
   "source": [
    "<a id='Q3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.3</b> The Target Network (10 points) </h3> \n",
    "\n",
    "In DQN, we use an additional target network to calculate the target Q value.\n",
    "    \n",
    "1.Why we need an additional target network ? \n",
    "    \n",
    "2.Can we just use the same network (no an additional target network ) in calculating both Q(s,a) and maxa(Q(s‚Ä≤,¬∑))? why or why not?\n",
    "    \n",
    "3.What will happen if we do not stop gradient of the target Q value?\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897854c-9516-49e0-a5bd-dbc79e0dcd63",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266d794-008d-4d2c-9a9e-0e18d9b6e354",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Submitting <a id='3.'></a>\n",
    "Ensure all tasks and questions (in ```ex4_dpn.ipynb``` and ```ex4_rbf.ipynb```) are answered and and that the necessary plots are saved in the appropriate locations. The relevant plots and files needed to be submitted for this assignment are:\n",
    "\n",
    "\n",
    "- Training performance plots:\n",
    "  - `cartpole_dqn.png`: Cartpole, training performance plots in terms of episode and episodic reward\n",
    "  - `lunarlander_dqn.png`: Lunarlander, training performance plots in terms of episode and episodic reward \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "  \n",
    "\n",
    "- Model files:\n",
    "  - `dqn.pt`: Trained model\n",
    "\n",
    "\n",
    "Ensure the model files and plots are saved in correct paths:\n",
    "- ```results/CartPole-v1/cartpole_dqn``` Training result for Cartpole environment\n",
    "- ```results/CartPole-v1/model/dqn.pt``` Model for Cartpole environment\n",
    "- ```results/LunarLander-v2/lunarlander_dqn.png``` Training result for LunarLander environment\n",
    "- ```results/LunarLander-v2/model/dqn.pt``` Model for LunarLander environment\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER TASK IN ```ex4_rbf.ipynb```** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec40b02-69c6-401a-ad79-2f67a8a660e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1 Feedback <a id='3.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer to the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a8baf6-b860-4338-b655-b28ec4fac282",
   "metadata": {},
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94026d-ad80-4148-8e02-5bcfd4b33e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e1e56-4c4f-4bdf-95b3-f4efcc4c053a",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f647d-4cf5-4cf8-abbf-1c0d285c5f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Implementing DQN (10 points)\n",
    "Q1_1 = None # Question 1.1 Considering Continuous Action Spaces (5 points)\n",
    "Q1_2 = None # Question 1.2 Continuous Action Spaces Part 1 (15 points)\n",
    "Q1_3 = None # Question 1.3 The Target Network (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb4a6d0-4ff1-4705-93af-493eed829e4e",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb003431-4a23-4350-86ec-34444a64ee9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Implementing DQN (10 points)\n",
    "Q1_1 = None # Question 1.1 Considering Continuous Action Spaces (5 points)\n",
    "Q1_2 = None # Question 1.2 Continuous Action Spaces Part 1 (15 points)\n",
    "Q1_3 = None # Question 1.3 The Target Network (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa9a62-dcf9-41fe-8d56-a852b2582efa",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "Please share any additional feedback, suggestions, or comments you have about the lecture, assignment, or course content. Your input is valuable in helping us improve the learning experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77edf97b-6cd9-48a9-bbc9-a075e6c15abb",
   "metadata": {
    "tags": []
   },
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
